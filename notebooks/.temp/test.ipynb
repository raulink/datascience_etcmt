{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "class GoogleSheetProcessor:\n",
    "    def __init__(self, sheet_url1: str):\n",
    "        self.sheet_url1 = sheet_url1\n",
    "        self.spreadsheet_id = self.extract_spreadsheet_id(sheet_url1)\n",
    "        self.sheet_id = self.extract_sheet_id(sheet_url1)\n",
    "        self.csv_export_url = self.construct_csv_export_url()\n",
    "\n",
    "    def extract_spreadsheet_id(self, url: str):\n",
    "        return url.split('/d/')[1].split('/')[0]\n",
    "\n",
    "    def extract_sheet_id(self, url: str):\n",
    "        return url.split('gid=')[1]\n",
    "\n",
    "    def construct_csv_export_url(self):\n",
    "        return f\"https://docs.google.com/spreadsheets/d/{self.spreadsheet_id}/export?format=csv&gid={self.sheet_id}\"\n",
    "\n",
    "    def download_csv(self, output_filename: str = 'temp_sheet.csv'):\n",
    "        # Descarga el archivo CSV y lo guarda temporalmente\n",
    "        response = requests.get(self.csv_export_url)\n",
    "        response.raise_for_status()  # Asegurarse de que la solicitud fue exitosa\n",
    "        with open(output_filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return pd.read_csv(output_filename)\n",
    "\n",
    "    def validate_no_duplicate_columns(self, df: pd.DataFrame):\n",
    "        # Validar si existen columnas duplicadas en el DataFrame\n",
    "        duplicate_columns = df.columns[df.columns.duplicated()]\n",
    "        if not duplicate_columns.empty:\n",
    "            raise ValueError(\n",
    "                f\"Columnas duplicadas encontradas: {duplicate_columns.tolist()}\")\n",
    "\n",
    "    def convertir_booleano_amef(self, df: pd.DataFrame, ini: int, end: int):\n",
    "        \"\"\"\n",
    "        Convierte a tipo booleano las columnas en el rango dado, si no son ya booleanas.\n",
    "\n",
    "        Parámetros:\n",
    "        - df: DataFrame en el que se realizará la conversión.\n",
    "        - ini: Índice de la columna inicial.\n",
    "        - end: Índice de la columna final.\n",
    "\n",
    "        Retorna:\n",
    "        - DataFrame con las columnas convertidas a booleano.\n",
    "        \"\"\"\n",
    "        # Iterar sobre las columnas en el rango especificado\n",
    "        for col in df.columns[ini + 1:end]:\n",
    "            # Verificar si la columna no es de tipo booleano\n",
    "            if df[col].dtype != bool:\n",
    "                # Convertir la columna a booleano\n",
    "                df[col] = df[col].map(lambda x: True if str(\n",
    "                    x).upper() == 'TRUE' else False)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def load_data_from_postgres(\n",
    "            self,\n",
    "            db_url: str = 'postgresql://postgres:postgres@localhost/simyo',\n",
    "            query: str = \"\"\"\n",
    "                SELECT\n",
    "                    base.\"id\",\n",
    "                    \"structure\".tag,\n",
    "                    locations.location_code,\n",
    "                    \"plans\".\"name\",\n",
    "                    base.fk_plan\n",
    "                FROM\n",
    "                    base\n",
    "                    INNER JOIN\n",
    "                    locations\n",
    "                    ON\n",
    "                        base.fk_location = locations.\"id\"\n",
    "                    INNER JOIN\n",
    "                    \"structure\"\n",
    "                    ON\n",
    "                        base.fk_structure = \"structure\".\"id\"\n",
    "                    LEFT JOIN\n",
    "                    \"plans\"\n",
    "                    ON\n",
    "                        base.fk_plan = \"plans\".\"id\"\n",
    "                \"\"\"):\n",
    "        \"\"\"\n",
    "        Conecta a la base de datos PostgreSQL, ejecuta la consulta SQL y devuelve un DataFrame.\n",
    "\n",
    "        Parámetros:\n",
    "        - db_url (str): URL de la base de datos PostgreSQL.\n",
    "        - query (str): Consulta SQL a ejecutar.\n",
    "\n",
    "        Retorna:\n",
    "        - DataFrame con los datos obtenidos de la consulta.\n",
    "        \"\"\"\n",
    "        # Crear un engine de SQLAlchemy para conectarse a la base de datos\n",
    "        engine = sqlalchemy.create_engine(db_url)\n",
    "\n",
    "        # Ejecutar la consulta y cargar los resultados en un DataFrame\n",
    "        df = pd.read_sql_query(query, engine)\n",
    "\n",
    "        # Establecer la columna 'id' como índice del DataFrame\n",
    "        df.index = df['id'].values\n",
    "\n",
    "        return df\n",
    "\n",
    "    def process_dataframe(self, df: pd.DataFrame, columns_to_remove: list):\n",
    "\n",
    "        # Si existen columnas con nombres duplicados, arrojar un error de columnas duplicadas\n",
    "        if df.columns.duplicated().any():\n",
    "            assert False, \"Hay columnas duplicadas en el DataFrame df1\"\n",
    "        # Eliminar columnas innecesarias\n",
    "        df = df.drop(columns=columns_to_remove, errors='ignore')\n",
    "\n",
    "        # Eliminar filas donde 'TIPO' tenga valores 'Sistema' o 'Subsistema'\n",
    "        df = df[~df['Tipo'].isin(['Sistema', 'Subsistema'])]\n",
    "\n",
    "        # Filtrar filas donde la columna 'Plan' no sea nula\n",
    "        df = df[df['Plan'].notna()]\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_output_dataframe(self, df: pd.DataFrame):\n",
    "        # Crear un DataFrame de salida con columnas específicas\n",
    "        # Iterar sobre las filas del dataframe\n",
    "        df_salida = pd.DataFrame()\n",
    "        for index, row in df.iterrows():\n",
    "            # Buscar el índice de la columna 'AMEF'\n",
    "            try:\n",
    "                amef_index = df.columns.get_loc('AMEF')\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            # Iterar a partir de la columna siguiente a 'AMEF'\n",
    "            for col in df.columns[amef_index + 1:]:\n",
    "                # print(col)\n",
    "                if row[col] == True:  # Si el valor es True\n",
    "                    # Adicionar una nueva fila al dataframe de salida\n",
    "                    new_row = pd.DataFrame({\n",
    "                        'tag': [row['Tag']],\n",
    "                        'location_code': [col],\n",
    "                        'plan': [row['Plan']]\n",
    "                    })\n",
    "                    df_salida = pd.concat(\n",
    "                        [df_salida, new_row], ignore_index=True)\n",
    "        return df_salida\n",
    "\n",
    "    def load_data_from_db(self, query: str, db_path: str):\n",
    "        # Cargar datos desde una base de datos SQLite\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        df_db = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "        return df_db\n",
    "\n",
    "    def read_csv(self, filename=\"temp_sheet.csv\", column_row=None, row_ini=None):\n",
    "        # Lee el archivo CSV usando pandas\n",
    "        # df = pd.DataFrame()\n",
    "        df = pd.read_csv(filename, header=column_row,\n",
    "                         skiprows=None, skipfooter=0)\n",
    "        if column_row and row_ini is None:\n",
    "            return df\n",
    "        else:\n",
    "            df.columns = df.loc[column_row, :].to_list()  # la fila 2 como fila\n",
    "            df = df.loc[row_ini:, :]   # Obtener desde la fila 4 en adelante\n",
    "            return df\n",
    "\n",
    "    def merge_dataframes(self, df1: pd.DataFrame, df2: pd.DataFrame, on_columns: list):\n",
    "        # Realizar el merge de dos DataFrames\n",
    "        return pd.merge(df1, df2, on=on_columns, how='left', suffixes=('_left', '_right'))\n",
    "\n",
    "    def process(\n",
    "            self,\n",
    "            df_input,\n",
    "            columns_to_remove: list = ['RO', 'AM', 'AZ', 'MO', 'VE', 'BL', 'NA', 'CE', 'CA', 'PL']):\n",
    "        # Método principal que engloba toda la lógica\n",
    "        # df = self.download_csv()  # Descarga y carga del CSV\n",
    "        # df = self.read_csv(filename=filename_input,column_row=2,row_ini=4)\n",
    "        # print(filename_input)\n",
    "        # df = pd.read_csv(filename_input)\n",
    "\n",
    "        # self.validate_no_duplicate_columns(df)  # Validar columnas duplicadas\n",
    "        # Método principal que engloba toda la lógica\n",
    "        df = self.process_dataframe(\n",
    "            df_input, columns_to_remove)  # Procesar el DataFrame\n",
    "        amef_index = df.columns.get_loc('AMEF')\n",
    "        end = len(df.columns)\n",
    "        df = self.convertir_booleano_amef(df, amef_index, end)\n",
    "        # Crear DataFrame de salida\n",
    "        df_output = self.create_output_dataframe(df)\n",
    "\n",
    "        cantidad_activos = df.iloc[:, amef_index + 1:].sum().sum()\n",
    "        print(f\"Existen {cantidad_activos} activos en la hoja de datos\")\n",
    "        if len(df_output) != cantidad_activos:\n",
    "            assert False, \"La cantidad de activos no corresponde con la salida\"\n",
    "        \n",
    "        # Cargar datos desde la base de datos\n",
    "        df_db = self.load_data_from_postgres()  \n",
    "        # Merge de DataFrames\n",
    "        df_result = self.merge_dataframes(df_output, df_db, ['tag', 'location_code'])  \n",
    "\n",
    "        return df_output, df_db, df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existen 12817 activos en la hoja de datos\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gs1 = GoogleSheetProcessor (\"https://docs.google.com/spreadsheets/d/1oUHkuKpHtuhMirNW6SvAQ4A0ns5PZs71iZ_WFXZHNn8/edit?gid=1115106678#gid=1115106678\")\n",
    "archivo = \"input/pad_2.csv\"\n",
    "#gs1.download_csv(archivo)\n",
    "df1 = pd.read_csv(archivo,header=3,true_values=['True','TRUE'],false_values=['False',\"FALSE\",\"\"])\n",
    "\n",
    "\n",
    "gs2 = GoogleSheetProcessor(\"https://docs.google.com/spreadsheets/d/1yOaSeqRBr1FW6tvFMi_Y-s4011cKBoyiWU5dTMlujrU/edit?gid=1115106678#gid=1115106678\")\n",
    "archivo = \"input/pbd_2.csv\"\n",
    "#gs2.download_csv(archivo)\n",
    "df2 = pd.read_csv(archivo,header=3,true_values=['True','TRUE'],false_values=['False',\"FALSE\",\"\"])\n",
    "\n",
    "df_merged = pd.concat([df1,df2],ignore_index=True).reset_index(drop=True)\n",
    "filename = \"input/mix_plan_2.csv\"\n",
    "df_merged.to_csv(filename,sep=';')\n",
    "\n",
    "df_output, df_db, df_result=gs1.process(df_merged)\n",
    "df_result.to_excel(\"Salida_Planes.xlsx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
