{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1423f6da5e4a1b85a1b894a68a7756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='https://docs.google.com/spreadsheets/d/1oUHkuKpHtuhMirNW6SvAQ4A0ns5PZs71iZ_WFXZHNn8/edit?gid=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810024a346084bbfabf85152cd1628c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='https://docs.google.com/spreadsheets/d/1yOaSeqRBr1FW6tvFMi_Y-s4011cKBoyiWU5dTMlujrU/edit?gid=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b01c9a8e76464eae4af0f777b9dd89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(Text(value='user_gom_mantenimiento', description='Usuario'), Password(description='Passwor…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b209ef50e2d04711a3b959e102b814a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Generar Archivo Excel', layout=Layout(width='20%'), style=ButtonSt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4da88a1e01447f9120ea192d7aa23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='danger', description='Cargar en Base de datos', layout=Layout(width='20%'), style=ButtonS…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e6e91fcd4e4be9a2f0cf3d35c9455f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(Output(),), titles=('',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20List.html\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Button, Layout\n",
    "from IPython.display import display\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# Variables\n",
    "lad = 'https://docs.google.com/spreadsheets/d/1oUHkuKpHtuhMirNW6SvAQ4A0ns5PZs71iZ_WFXZHNn8/edit?gid=1199302294'\n",
    "lbd = 'https://docs.google.com/spreadsheets/d/1yOaSeqRBr1FW6tvFMi_Y-s4011cKBoyiWU5dTMlujrU/edit?gid=1199302294'\n",
    "host = '192.168.100.56'#'localhost'#'192.168.100.50'\n",
    "basedatos= 'db_mantenimiento' #'db_mantenimiento'\n",
    "usuario = 'user_gom_mantenimiento' #'postgres'\n",
    "password = 'M4ntenim13nto.' #''\n",
    "salida = \"Salida.xlsx\"\n",
    "# Bloque de variables\n",
    "descargar = False\n",
    "url_alta_demanda =lad\n",
    "archivo_ad = \"input/pad_actividades.csv\"\n",
    "url_baja_demanda = lbd\n",
    "archivo_bd = \"input/pbd_actividades.csv\"\n",
    "filename_actividades = \"input/mix_plan_actividades.csv\"\n",
    "\n",
    "valores = {\n",
    "    \"D\": 1,\n",
    "    \"S\": 1,\n",
    "    \"2S\": 2,\n",
    "    \"M\": 5,\n",
    "    \"MC\": 1,\n",
    "    \"2M\": 2,\n",
    "    \"T\": 3,\n",
    "    \"SE\": 6,\n",
    "    \"8M\": 8,\n",
    "    \"A\": 1,\n",
    "    \"1.5A\": 18,\n",
    "    \"2A\": 2,\n",
    "    \"3A\": 3,\n",
    "    \"4A\": 4,\n",
    "    \"5A\": 5,\n",
    "    \"6A\": 6,\n",
    "    \"8A\": 8,\n",
    "    \"10A\": 10,\n",
    "    \"1000\": 1000,\n",
    "    \"1300\": 1300,\n",
    "    \"1800\": 1800,\n",
    "    \"6000\": 6000,\n",
    "    \"22500\": 6000,\n",
    "    \"40000\": 40000,\n",
    "    \"55000\": 55000,\n",
    "    \"55000C\": 55000}\n",
    "regimen = {\n",
    "    \"D\": 'dia(s)',\n",
    "    \"S\": 'semana(s)',\n",
    "    \"2S\": 'semana(s)',\n",
    "    \"M\": 'semana(s)',\n",
    "    \"MC\": 'mes(es)',\n",
    "    \"2M\": 'mes(es)',\n",
    "    \"T\": 'mes(es)',\n",
    "    \"SE\": 'mes(es)',\n",
    "    \"8M\": 'mes(es)',\n",
    "    \"A\": 'año(s)',\n",
    "    \"1.5A\": 'mes(es)',\n",
    "    \"2A\": 'año(s)',\n",
    "    \"3A\": 'año(s)',\n",
    "    \"4A\": 'año(s)',\n",
    "    \"5A\": 'año(s)',\n",
    "    \"6A\": 'año(s)',\n",
    "    \"8A\": 'año(s)',\n",
    "    \"10A\": 'año(s)',\n",
    "    \"1000\": 'horas',\n",
    "    \"1300\": 'horas',\n",
    "    \"1800\": 'horas',\n",
    "    \"6000\": 'horas',\n",
    "    \"22500\": 'horas',\n",
    "    \"40000\": 'horas',\n",
    "    \"55000\": 'horas',\n",
    "    \"55000C\": 'ciclos'\n",
    "}\n",
    "\n",
    "wlad = widgets.Textarea(value=lad,placeholder='Plan Maestro LAD',description='Lineas Alta Demanda:',disabled=False,layout=Layout(width='70%',height=\"200px\"))\n",
    "wlbd = widgets.Textarea(value=lbd,placeholder='Plan Maestro LBD',description='Lineas Baja Demanda:',disabled=False,layout=Layout(width='70%',height=\"200px\"))\n",
    "whost = widgets.Text(value=host,placeholder='Host',description='Host:',disabled=False)\n",
    "wbasedatos = widgets.Text(value=basedatos,placeholder='BaseDatos',description='BaseDatos',disabled=False)\n",
    "wusuario = widgets.Text(value=usuario,description='Usuario')\n",
    "wpassword = widgets.Password(value=password,description='Password')\n",
    "wbutton1 = widgets.Button(description=\"Generar Archivo Excel\",button_style='success',layout=Layout(width='20%'))\n",
    "wbutton2 = widgets.Button(description=\"Cargar en Base de datos\",button_style='danger',layout=Layout(width='20%'))\n",
    "woutput = widgets.Output()\n",
    "wsalida = widgets.Text(value=salida,description=\"Nombre:\",disabled=False)\n",
    "\n",
    "waccordion = widgets.Accordion(children=[ wusuario,wpassword,whost,wbasedatos], titles=('Usuario','Password','Host','Base de Datos'))\n",
    "wsalida = widgets.Accordion(children=[ woutput])\n",
    "\n",
    "display(wlad,wlbd,waccordion,wbutton1,wbutton2,wsalida)\n",
    "\n",
    "def on_button_clicked(b):                \n",
    "    with woutput:\n",
    "        print(\"Ejecucion Script Fase 1\")\n",
    "\n",
    "def on_button_clicked_2(b):                \n",
    "    with woutput:\n",
    "        print(\"Ejecucion Script Fase 2\")\n",
    "\n",
    "wbutton1.on_click(on_button_clicked)\n",
    "wbutton2.on_click(on_button_clicked_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script de migracion de datos\n",
    "\n",
    "Este programa está diseñado para procesar datos de, en el caso de los planes de mantenimiento, se tienen 2 planes de mantenimiento (Lineas de Alta Demanda LAD y Lineas de Baja Demanda LBD) descargándolos, transformándolos y guardándolos en un archivo Excel. La clase principal, GoogleSheetProcessor, maneja todo el flujo de trabajo, desde la obtención de datos hasta su procesamiento y almacenamiento. A continuación se describe el funcionamiento detallado del programa.\n",
    "\n",
    "## Funcionalidades Principales\n",
    "1. Inicialización (__init__)\n",
    "La clase se inicializa con la URL de una hoja de cálculo de Google Sheets. Durante la inicialización, se extraen los identificadores del archivo y de la hoja específica, y se construye la URL para exportar la hoja en formato CSV.\n",
    "\n",
    "### Atributos:\n",
    "\n",
    "sheet_url: URL de la hoja de cálculo de Google Sheets.\n",
    "spreadsheet_id: ID único de la hoja de cálculo.\n",
    "sheet_id: ID único de la hoja dentro de la hoja de cálculo.\n",
    "csv_export_url: URL para exportar la hoja en formato CSV.\n",
    "\n",
    "### Diccionarios predefinidos:\n",
    "\n",
    "valores: Diccionario que asocia códigos con valores numéricos. Este diccionario de valores es dependiendo las columnas de la hoja de calculo, especifico a las frecuencias \n",
    "regimen: Diccionario que asocia códigos con sus unidades de medida correspondientes.\n",
    "\n",
    "\n",
    "1. Extracción de Identificadores\n",
    "extract_spreadsheet_id(url): Extrae el ID de la hoja de cálculo desde la URL.\n",
    "extract_sheet_id(url): Extrae el ID de la hoja específica desde la URL.\n",
    "2. Construcción de la URL de Exportación\n",
    "construct_csv_export_url(): Construye la URL que permite descargar la hoja de cálculo en formato CSV.\n",
    "3. Descarga de la Hoja en CSV\n",
    "download_csv(output_filename='temp_sheet.csv'): Descarga la hoja de cálculo en formato CSV y la guarda con un nombre de archivo especificado (por defecto, temp_sheet.csv).\n",
    "4. Procesamiento de Datos\n",
    "process_data(filename=\"temp_sheet.csv\", valores=\"\", regimen=\"\"):\n",
    "Carga el archivo CSV y realiza una serie de transformaciones y filtrados en los datos, como la conversión de valores, la asignación de unidades, y la reestructuración del DataFrame.\n",
    "Comprueba que las claves de los diccionarios valores y regimen coinciden, lanzando un AssertionError si no es así.\n",
    "Filtra los datos para excluir planes y mantener solo las columnas relevantes.\n",
    "Realiza transformaciones en el DataFrame para preparar la información que se almacenará en Excel.\n",
    "Retorna los DataFrames df_plan, df_action, df_speciality y filtered_data.\n",
    "5. Guardar Resultados en Excel\n",
    "save_to_excel(output_path=\"Salida.xlsx\", filename=\"mix_plan.csv\", valores=\"\", regimen=\"\"):\n",
    "Llama al método process_data para obtener los DataFrames procesados.\n",
    "Guarda los DataFrames resultantes en un archivo Excel con hojas separadas para acciones, planes, especialidades y actividades filtradas.\n",
    "6. Funciones Auxiliares\n",
    "get_unique(df: pd.DataFrame, column: str): Genera un DataFrame con valores únicos de una columna específica, ajustando los índices.\n",
    "buscarIndice(df: pd.DataFrame, valor, columna='value'): Busca el índice de un valor específico en un DataFrame y lo retorna como un entero.\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Inicio] --> B[Inicializa GoogleSheetProcessor con lad.value]\n",
    "    B --> C[Lee CSV desde input/pad.csv]\n",
    "    C --> D[Inicializa GoogleSheetProcessor con lbd.value]\n",
    "    D --> E[Lee CSV desde input/pbd.csv]\n",
    "    E --> F[Concatena los DataFrames df1 y df2]\n",
    "    F --> G[Guarda el DataFrame combinado indicado en el campo salida]    \n",
    "    G --> H[Guarda el Dataframe directamente en la base de datos indicada]\n",
    "\n",
    "```\n",
    "\n",
    "Descripción del Flujograma\n",
    "* Inicio: El proceso comienza con la inicialización del primer GoogleSheetProcessor con la URL contenida en lad.value.\n",
    "* Lectura del primer CSV: Se lee el archivo CSV asociado al primer DataFrame desde la ruta input/pad.csv.\n",
    "* Inicialización del segundo GoogleSheetProcessor: Se inicializa el segundo objeto GoogleSheetProcessor con la URL contenida en lbd.value.\n",
    "* Lectura del segundo CSV: Se lee el archivo CSV asociado al segundo DataFrame desde la ruta input/pbd.csv.\n",
    "* Concatenación de DataFrames: Los dos DataFrames (df1 y df2) se combinan en uno solo mediante pd.concat.\n",
    "* Guardar el DataFrame combinado: El DataFrame combinado se guarda en un archivo CSV en la ruta input/mix_plan.csv.\n",
    "* Procesamiento y Almacenamiento de hoja de calculo procesada: Se puede almacenar en archivo excel o directamente en la base de datos, \n",
    "\n",
    "\n",
    "### Pantalla 1\n",
    "Al presionar el boton de _Generar Archivo Excel_ se realiza la generación del archivo excel en la carpeta output, colocando el nombre del archivo excel que se encuetnra en el campo de salida.\n",
    "\n",
    "![Pantalla 1](assets/pantalla1.png \"Pantalla 1\")\n",
    "\n",
    "### Pantalla 2\n",
    "\n",
    "En este apartado se colocan los datos de conexion de la base de datos. Al presionar el botón de _Cargar en Base de datos_ se procede a conectar y a volcar el dataframe en la base de datos indicada en los datos de usuario, password, host y base de datos de destino.\n",
    "\n",
    "![Pantalla 2](assets/pantalla2.png \"Pantalla 2\")\n",
    "\n",
    "\n",
    "\n",
    "> Nota. Exportar directamente en la base de datos aun no es posible, se requiere complementar el codigo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "class GoogleSheetProcessor1:\n",
    "    def __init__(self, sheet_url:str):\n",
    "        self.sheet_url = sheet_url\n",
    "        self.spreadsheet_id = self.extract_spreadsheet_id(sheet_url)\n",
    "        self.sheet_id = self.extract_sheet_id(sheet_url)\n",
    "        self.csv_export_url = self.construct_csv_export_url()\n",
    "\n",
    "        # Diccionarios originales\n",
    "        self.valores = {\n",
    "            \"D\": 1, \"S\": 1, \"M\": 5, \"MC\": 1, \"2M\": 2, \"T\": 3, \"4M\": 4, \"SE\": 6,\n",
    "            \"8M\": 8, \"A\": 1, \"1.5A\": 18, \"2A\": 2, \"3A\": 3, \"4A\": 4, \"5A\": 5,\n",
    "            \"6A\": 6, \"8A\": 8, \"10A\": 10, \"1000\": 1000, \"6000\": 6000, \"22500\": 22500,\n",
    "            \"40000\": 40000, \"55000\": 55000\n",
    "        }\n",
    "\n",
    "        self.regimen = {\n",
    "            \"D\": 'dia', \"S\": 'semana', \"M\": 'semana', \"MC\": 'mes', \"2M\": 'mes', \"T\": 'mes',\n",
    "            \"4M\": 'mes', \"SE\": 'mes', \"8M\": 'mes', \"A\": 'año', \"1.5A\": 'mes', \"2A\": 'año',\n",
    "            \"3A\": 'año', \"4A\": 'año', \"5A\": 'año', \"6A\": 'año', \"8A\": 'año', \"10A\": 'año',\n",
    "            \"1000\": 'horas', \"6000\": 'horas', \"22500\": 'horas', \"40000\": 'horas', \"55000\": 'horas'\n",
    "        }\n",
    "\n",
    "    def extract_spreadsheet_id(self, url):\n",
    "        return url.split('/d/')[1].split('/')[0]\n",
    "\n",
    "    def extract_sheet_id(self, url):\n",
    "        return url.split('gid=')[1]\n",
    "\n",
    "    def construct_csv_export_url(self):\n",
    "        return f\"https://docs.google.com/spreadsheets/d/{self.spreadsheet_id}/export?format=csv&gid={self.sheet_id}\"\n",
    "\n",
    "    def download_csv(self, output_filename='temp_sheet.csv'):\n",
    "        # Descarga el archivo CSV y lo guarda temporalmente\n",
    "        response = requests.get(self.csv_export_url)\n",
    "        response.raise_for_status()  # Asegurarse de que la solicitud fue exitosa\n",
    "        with open(output_filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return output_filename\n",
    "    def get_unique(self, df: pd.DataFrame, column: str):\n",
    "        \"\"\"\n",
    "        Obtiene un DataFrame con valores únicos de la columna 'Column', con índices ajustados.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: Un DataFrame con valores únicos de la columna 'Column' y un índice ajustado.\n",
    "        \"\"\"\n",
    "        df[column] = df[column].str.strip()\n",
    "        df = df[df[column].notnull()]\n",
    "        df_unique = pd.DataFrame(df[column].unique(), columns=['value'])\n",
    "        df_unique.index = df_unique.index + 1\n",
    "        return df_unique\n",
    "\n",
    "\n",
    "    def buscarIndice(self, df: pd.DataFrame, valor:str, columna_id='value'):\n",
    "        # Verificar si el valor está en la columna_id especificada\n",
    "        \"\"\"\n",
    "        Busca un valor en la columna especificada del DataFrame.\n",
    "        Si el valor de búsqueda es nulo o si no se encuentra, retorna el mismo valor de búsqueda.\n",
    "        \"\"\"\n",
    "        # Validar si el valor de búsqueda es nulo        \n",
    "            \n",
    "        if pd.isna(valor):\n",
    "            return None #pd.NA    \n",
    "        \n",
    "        #fkc_priority tiene el valor \"BAJA \" con espacio al final, eliminar el ultimo espacio Se añade a la funcion buscarIndice\n",
    "        valor = valor.upper().strip()\n",
    "\n",
    "        resultado = df[df[columna_id].str.upper() == valor]    \n",
    "        # Si no encuentra el valor, retornar el mismo valor\n",
    "        if resultado.empty:\n",
    "            return valor\n",
    "        else:\n",
    "            return int(resultado.index[0])\n",
    "\n",
    "\n",
    "    def read_csv(self, filename=\"temp_sheet.csv\"):\n",
    "        # Lee el archivo CSV usando pandas\n",
    "        self.df = pd.read_csv(filename)\n",
    "        self.df.columns = self.df.loc[2, :].to_list()  # la fila 2 como fila\n",
    "        self.df = self.df.loc[4:, :]   # Obtener desde la fila 4 en adelante\n",
    "        return self.df\n",
    "    \n",
    "    def process_data_with_validation(self, df: pd.DataFrame, valores: dict):\n",
    "        # Iterar sobre las claves del diccionario valores\n",
    "        for col in valores.keys():\n",
    "            # Verificar si la columna existe en el DataFrame\n",
    "            if col in df.columns:\n",
    "                # Comprobar si la columna no es booleana\n",
    "                if not pd.api.types.is_bool_dtype(df[col]):\n",
    "                    # Si no es booleana, intentamos convertirla\n",
    "                    df[col] = df[col].apply(lambda x: True if str(x).upper() == 'TRUE' else False)\n",
    "                    # df[list(valores.keys())] = df[valores.keys()].applymap(lambda x: True if x == 'TRUE' else False)\n",
    "            else:\n",
    "                print(f\"La columna '{col}' no se encuentra en el DataFrame.\")\n",
    "        return df\n",
    "\n",
    "    def process_data(self,filename = \"temp_sheet.csv\",valores=\"\", regimen=\"\"):        \n",
    "        df = pd.DataFrame()\n",
    "        df = pd.read_csv(filename)\n",
    "\n",
    "        if valores == \"\":valores = self.valores    \n",
    "        if regimen == \"\":regimen = self.regimen\n",
    "        \n",
    "        if valores.keys() != regimen.keys():\n",
    "            raise AssertionError(f\"Las claves no coinciden: {valores.keys()} != {regimen.keys()}\")\n",
    "\n",
    "        # Realiza el procesamiento necesario\n",
    "        # Este es un lugar para incluir toda la lógica de procesamiento\n",
    "        \n",
    "        # Suponiendo que el procesamiento produce 'filtered_data' y otros DataFrames\n",
    "        df_plan = pd.DataFrame()  # Placeholder\n",
    "        df_action = pd.DataFrame()  # Placeholder\n",
    "        df_speciality = pd.DataFrame()  # Placeholder\n",
    "        filtered_data = pd.DataFrame()  # Placeholder        \n",
    "        #print(valores)\n",
    "        ## convertir a booleano\n",
    "        # df[list(valores.keys())] = df[valores.keys()].applymap(lambda x: True if x == 'TRUE' else False)\n",
    "        ## convertir a booleano\n",
    "        df = self.process_data_with_validation(df,valores)\n",
    "        #print(df.dtypes)\n",
    "\n",
    "        # Quitar planes\n",
    "        df = df[df['Tipo_plan']!= 'Plan']   # Se cambio de Tipo a Tipo_plan el 5-9-24\n",
    "\n",
    "        # Obtener la unidades\n",
    "        parametros = regimen\n",
    "        df['unidad'] = df.apply(lambda row: next((parametros[key] for key in parametros.keys() if key in row and row[key] == True), None), axis=1)\n",
    "        # Obtener los valores\n",
    "        parametros = valores\n",
    "        df['valor'] = df.apply(lambda row: next((parametros[key] for key in parametros.keys() if key in row and row[key] == True), None), axis=1)\n",
    "        # Filtrar las columnas necesarias solamente\n",
    "        #print(\"Valores unicos en unidades: \")\n",
    "        #print(df['unidad'].unique())        \n",
    "        # Mantener solo las columnas necesarias\n",
    "        columns = ['Plan','Accion','Trabajo','Actividad','Tipo_plan','Parada','Relevancia','Especialidad','valor','unidad']\n",
    "        df = df[columns]\n",
    "        # Crear la nueva columna fk_activity que tendra relaciones con las actividades padre\n",
    "        df['fk_activity']= None\n",
    "        df['fkc_regime']= None\n",
    "\n",
    "        # renombrar los nombres de las columnas\n",
    "        nuevos_nombres = {\n",
    "            'Plan': 'fk_plan',\n",
    "            'Accion': 'fk_action',\n",
    "            'Actividad': 'name',\n",
    "            'Tipo_plan': 'fkc_activity_type',\n",
    "            'Relevancia': 'fkc_priority',\n",
    "            'Especialidad': 'fk_specialty',\n",
    "            'valor': 'time_interval_value',\n",
    "            'unidad': 'fk_periodicity_unit',\n",
    "            'Parada': 'stoppage',\n",
    "        }\n",
    "        df.rename(columns=nuevos_nombres, inplace=True)\n",
    "                # Mantener las columnas del excel en el orden indicado\n",
    "        columnas_excel = ['fk_activity','fk_plan','fk_action','name','fkc_activity_type','fkc_priority','fk_specialty','fkc_regime','stoppage','time_interval_value','fk_periodicity_unit'] \n",
    "\n",
    "        df = df[columnas_excel]\n",
    "        df_plan = self.get_unique(df,\"fk_plan\")\n",
    "        df_action = self.get_unique(df,\"fk_action\")\n",
    "        df_speciality = self.get_unique(df,\"fk_specialty\")\n",
    "        df_activity_type = self.get_unique(df,\"fkc_activity_type\")\n",
    "        df_regime = self.get_unique(df,\"fkc_regime\")\n",
    "\n",
    "        # Filter the data\n",
    "        #df = df_raw.copy(deep=True)\n",
    "        filtered_data = df[(df['fkc_activity_type'] == 'Actividad') | (df['fkc_activity_type'] == 'Tarea')]\n",
    "        \n",
    "        # Add fk_activity column\n",
    "        filtered_data['fk_activity'] = None\n",
    "\n",
    "        # Buscar fk_activity para las Tareas que provienen de una Actividad\n",
    "        parent_index = None\n",
    "        for i, row in filtered_data.iterrows():\n",
    "            if row['fkc_activity_type'] == 'Actividad':\n",
    "                parent_index = i\n",
    "            elif row['fkc_activity_type'] == 'Tarea':\n",
    "                filtered_data.at[i, 'fk_activity'] = parent_index\n",
    "\n",
    "        # Obtener los ids de la relacion con los otros dataframes\n",
    "        filtered_data['fk_plan']= filtered_data['fk_plan'].apply(lambda x: self.buscarIndice(df_plan,x))\n",
    "        filtered_data['fk_action']= filtered_data['fk_action'].apply(lambda x: self.buscarIndice(df_action,x)) \n",
    "        filtered_data['fk_specialty']= filtered_data['fk_specialty'].apply(lambda x: self.buscarIndice(df_speciality,x)) \n",
    "        valores_lecturas=['horas','ciclos']\n",
    "        # Discriminar si las lecturas son horas o ciclos colocar FECHAS O LECTURAS\n",
    "        filtered_data['fkc_regime'] = filtered_data['fk_periodicity_unit'].apply(lambda x: 'LECTURAS' if x in valores_lecturas else 'FECHAS')\n",
    "        # Filtrar y aplicar los cambios correspondientes, mover los valores a las columnas de uso\n",
    "        filtered_data.loc[filtered_data['fk_periodicity_unit'].isin(valores_lecturas), 'usage_interval_value'] = filtered_data['time_interval_value']\n",
    "        filtered_data.loc[filtered_data['fk_periodicity_unit'].isin(valores_lecturas), 'fk_usage_unit'] = filtered_data['fk_periodicity_unit']\n",
    "\n",
    "        # Colocar los valores en las columnas timer_interva_value y fk_periodicity_unit en nulo\n",
    "        filtered_data.loc[filtered_data['fk_periodicity_unit'].isin(valores_lecturas), 'time_interval_value'] = None #pd.NA\n",
    "        filtered_data.loc[filtered_data['fk_periodicity_unit'].isin(valores_lecturas), 'fk_periodicity_unit'] = None #pd.NA\n",
    "       \n",
    "        return df_plan, df_action, df_speciality, filtered_data    \n",
    "\n",
    "\n",
    "    def save_to_excel(self, output_path=\"Salida.xlsx\",filename=\"mix_plan.csv\",valores=\"\",regimen=\"\"):\n",
    "        df_plan, df_action, df_speciality, filtered_data = self.process_data(filename=filename,valores=valores,regimen=regimen)  \n",
    "        with pd.ExcelWriter(output_path) as writer:\n",
    "            df_action.to_excel(writer, sheet_name='actions')\n",
    "            df_plan.to_excel(writer, sheet_name='plans')\n",
    "            df_speciality.to_excel(writer, sheet_name='specialties')\n",
    "            filtered_data.to_excel(writer, sheet_name='activities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinacion de dataframes\n",
    "gs1 = GoogleSheetProcessor1(url_alta_demanda)\n",
    "if descargar : gs1.download_csv(archivo_ad)\n",
    "df1 = gs1.read_csv(archivo_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs2 = GoogleSheetProcessor1 (url_baja_demanda)\n",
    "if descargar: gs2.download_csv(archivo_bd)\n",
    "df2 = gs2.read_csv(archivo_bd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([           'id',           'Cod',          'Plan',        'Accion',\n",
       "           'Actividad',     'Tipo_plan',        'Parada',    'Relevancia',\n",
       "        'Especialidad',             'D',             'S',            '2S',\n",
       "                   'M',            'MC',            '2M',             'T',\n",
       "                  'SE',            '8M',             'A',          '1.5A',\n",
       "                  '2A',            '3A',            '4A',            '5A',\n",
       "                  '6A',            '8A',           '10A',          '1000',\n",
       "                '1300',          '1800',          '6000',         '22500',\n",
       "               '40000',         '55000',        '55000C',  'Verificacion',\n",
       "             'Trabajo', 'Observaciones',             nan,           False],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df1.columns[:-2] == df2.columns[:-2]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar la igualdad de las columnas, menos las 2 ultimas columnas\n",
    "if (~(df1.columns[:-2] == df2.columns[:-2]).all()) :\n",
    "    print(\"No son iguales las columnas, verificar la igualdad de columnas\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar el merge de ambos planes en un solo dataframe\n",
    "df_merged = pd.concat([df1,df2],ignore_index=True).reset_index(drop=True)\n",
    "df_merged.to_csv(filename_actividades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plan, df_action, df_speciality, df_activities = gs1.process_data(filename=filename_actividades,valores=valores,regimen=regimen)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fk_activity              object\n",
       "fk_plan                   int64\n",
       "fk_action                 int64\n",
       "name                     object\n",
       "fkc_activity_type        object\n",
       "fkc_priority             object\n",
       "fk_specialty              int64\n",
       "fkc_regime               object\n",
       "stoppage                 object\n",
       "time_interval_value     float64\n",
       "fk_periodicity_unit      object\n",
       "usage_interval_value    float64\n",
       "fk_usage_unit            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_activities.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones principales (Para Base de Datos)\n",
    "1. **format_dataframe**:\n",
    "Descripción: Formatea un DataFrame antes de insertarlo en una base de datos, asegurándose de que tenga todas las columnas requeridas y ajustando algunas de sus propiedades (como agregar UUIDs, establecer columnas de seguimiento como created_at, updated_at, etc.).\n",
    "Uso: Asegura que las columnas entre el DataFrame y la tabla de la base de datos sean consistentes.\n",
    "2. **actualizar_tabla_postgres:**\n",
    "Descripción: Elimina los registros de una tabla PostgreSQL, inserta nuevos datos desde un DataFrame, y reinicia la secuencia de la columna id para evitar conflictos.\n",
    "Uso: Se utiliza para actualizar completamente una tabla en PostgreSQL con nuevos datos, manteniendo la consistencia de la columna id.\n",
    "3. **obtener_registros:**\n",
    "Descripción: Ejecuta una consulta SELECT * en una tabla PostgreSQL y devuelve los resultados en un DataFrame. Puede filtrar por columnas específicas si se le proporcionan.\n",
    "Uso: Sirve para obtener los registros de una tabla en formato DataFrame.\n",
    "4. **buscarIndice:**\n",
    "Descripción: Busca un valor en una columna específica de un DataFrame y devuelve el índice del primer resultado encontrado. Si no se encuentra el valor, devuelve el valor de búsqueda original.\n",
    "Uso: Para encontrar la posición de un valor en un DataFrame.\n",
    "5. **ejecutar_query:**\n",
    "Descripción: Ejecuta una consulta SQL (que puede o no devolver filas) y devuelve el resultado en un DataFrame si aplica.\n",
    "Uso: Ejecuta cualquier consulta SQL genérica, devolviendo resultados si es necesario.\n",
    "6. **eliminar_registros:**\n",
    "Descripción: Elimina todos los registros de una tabla PostgreSQL.\n",
    "Uso: Se utiliza para limpiar una tabla antes de insertar nuevos datos.\n",
    "7. **update_plans_table:**\n",
    "Descripción: Elimina registros de una tabla PostgreSQL, inserta nuevos datos desde un DataFrame y reinicia la secuencia de la columna id. Se enfoca en tablas relacionadas con planes.\n",
    "Uso: Función similar a actualizar_tabla_postgres, diseñada específicamente para actualizar tablas de planes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.engine.result import Result\n",
    "import pandas as pd\n",
    "import uuid\n",
    "\"\"\" Para pasar a la base de datos\"\"\"\n",
    "\n",
    "\n",
    "def format_dataframe(df:pd.DataFrame,tabla:str,\n",
    "                     usuario=usuario, password=password,host=host, database=basedatos):\n",
    "#def format_dataframe(df:pd.DataFrame,tabla:str,usuario='user_mantenimiento', password='pass_M4ntenimient0',host='192.168.100.56', database='db_mantenimiento_test'):\n",
    "    \n",
    "    # Crear el engine de SQLAlchemy\n",
    "    engine = create_engine(f'postgresql://{usuario}:{password}@{host}/{database}')\n",
    "    # Leer la tabla original en un DataFrame de pandas\n",
    "    df_origen = pd.read_sql_query(f\"SELECT * FROM {tabla}\", con=engine)\n",
    "\n",
    "    # Renombrar la columna 'value' a 'name' si existe\n",
    "    if 'value' in df.columns:\n",
    "        df = df.rename(columns={'value': 'name'})\n",
    "\n",
    "    # Si existe la columna 'uuid' en la tabla original, crear esa columna\n",
    "    if 'uuid' in df_origen.columns:\n",
    "        df['uuid'] = [str(uuid.uuid4()) for _ in range(len(df))]\n",
    "\n",
    "    if 'is_active' in df_origen.columns:\n",
    "        df['is_active'] = True        \n",
    "\n",
    "    # Identificar las columnas que están en df_origen pero no en df\n",
    "    missing_columns = [col for col in df_origen.columns if col not in df.columns]\n",
    "\n",
    "    # Añadir las columnas faltantes a df con valores NaN\n",
    "    for col in missing_columns:\n",
    "        df[col] = None #pd.NA  # O usa otro valor predeterminado si es necesario\n",
    "\n",
    "    # Añadir columnas comunes\n",
    "    df['id'] = df.index +1 if df.index[0] ==0 else df.index\n",
    "    df['created_by'] = 1\n",
    "    df['updated_by'] = 1\n",
    "    df['created_at'] = pd.Timestamp.now()\n",
    "    df['updated_at'] = pd.Timestamp.now()\n",
    "    # Validar que las columnas de df y df_origen sean iguales\n",
    "    columnas_df = set(df.columns)\n",
    "    columnas_df_origen = set(df_origen.columns)\n",
    "    \n",
    "    # Si las columnas no son iguales, lanzar un error\n",
    "    assert columnas_df == columnas_df_origen, f\"Las columnas no coinciden. Columnas faltantes: {columnas_df_origen - columnas_df} en df y {columnas_df - columnas_df_origen} en df_origen\"        \n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def actualizar_tabla_postgres(df: pd.DataFrame, tabla: str, columna_id: str,\n",
    "                              usuario=usuario, password=password,host=host, database=basedatos):    \n",
    "    # Reemplazar NaN por None (que en SQL es equivalente a NULL)\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "    # Crear el engine de SQLAlchemy\n",
    "    engine = create_engine(f'postgresql://{usuario}:{password}@{host}/{database}')\n",
    "        \n",
    "    # Eliminar todos los registros de la tabla\n",
    "    with engine.connect() as connection:\n",
    "        #connection.execute(text(f\"DELETE FROM {tabla};\"))\n",
    "        connection.execute(text(f\"DELETE FROM {tabla} CASCADE;\"))\n",
    "        connection.commit()\n",
    "\n",
    "    # Insertar los nuevos datos en la tabla\n",
    "    df.to_sql(tabla, con=engine, if_exists='append', index=False)  # Solo append en tablas con relaciones    \n",
    "\n",
    "    # Obtener el valor máximo de la columna 'id'\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(f\"SELECT MAX({columna_id}) FROM {tabla};\"))\n",
    "        max_id = result.scalar() or 0  # Si no hay registros, usar 0\n",
    "\n",
    "    # Obtener el nombre de la secuencia asociada a la columna 'id'\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(f\"SELECT pg_get_serial_sequence('{tabla}', '{columna_id}');\"))\n",
    "        id_secuencia = result.scalar()\n",
    "\n",
    "    # Reiniciar el valor de la secuencia si se obtiene la secuencia asociada\n",
    "    with engine.connect() as connection:\n",
    "        if id_secuencia:\n",
    "            connection.execute(text(f\"ALTER SEQUENCE {id_secuencia} RESTART WITH {max_id + 1};\"))\n",
    "            connection.commit()\n",
    "            print(f'Se reinició el índice {id_secuencia} en {max_id + 1}')\n",
    "\n",
    "# Ejemplo de uso:\n",
    "## actualizar_tabla_postgres(df_plan, 'plans', 'id')\n",
    "def obtener_registros( tabla,  usuario=usuario, password=password,host=host, database=basedatos,columna_ids=[]):\n",
    "    \"\"\"\n",
    "    Realiza un SELECT * en una tabla especificada de la base de datos y retorna un DataFrame con los resultados.\n",
    "    \"\"\"\n",
    "    # Crear el engine de SQLAlchemy\n",
    "    engine = create_engine(f'postgresql://{usuario}:{password}@{host}/{database}')\n",
    "    \n",
    "    with engine.connect() as connection:\n",
    "        # Ejecutar la sentencia SQL para obtener los registros\n",
    "        result = connection.execute(text(f'select * from {tabla}'))        \n",
    "        # Convertir los resultados en un DataFrame\n",
    "        df = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "        #df.index = df['index']\n",
    "        df.index = df['id']\n",
    "    return df if not columna_ids or len(columna_ids) == 0 else df[columna_ids]\n",
    "    \n",
    "def buscarIndice(df: pd.DataFrame, valor_busqueda:str, columna_busqueda='name'):\n",
    "    \"\"\"\n",
    "    Busca un valor en la columna especificada del DataFrame.\n",
    "    Si el valor de búsqueda es nulo o si no se encuentra, retorna el mismo valor de búsqueda.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Validar si el valor de búsqueda es nulo\n",
    "    if pd.isna(valor_busqueda):\n",
    "        return None #pd.NA    \n",
    "    # Verificar si el valor está en la columna_id especificada\n",
    "    resultado = df[df[columna_busqueda].str.contains(valor_busqueda, case=False, na=False)]    \n",
    "    # Si no encuentra el valor, retornar el mismo valor\n",
    "    if resultado.empty:\n",
    "        return valor_busqueda\n",
    "    else:\n",
    "        return int(resultado.index[0])\n",
    "\n",
    "def ejecutar_query(query, usuario=usuario, password=password,host=host, database=basedatos):\n",
    "    \"\"\"\n",
    "    Ejecuta una consulta SQL y devuelve el resultado en un DataFrame si la consulta devuelve filas.\n",
    "    \"\"\"\n",
    "    # Crear el engine de SQLAlchemy\n",
    "    engine = create_engine(f'postgresql://{usuario}:{password}@{host}/{database}')\n",
    "    \n",
    "    with engine.connect() as connection:\n",
    "        # Ejecutar la consulta\n",
    "        result = connection.execute(text(query))\n",
    "        connection.commit()\n",
    "        \n",
    "        # Verificar si la consulta devuelve filas\n",
    "        if result.returns_rows:\n",
    "            # Obtener los resultados en un DataFrame\n",
    "            df = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "            return df\n",
    "        else:\n",
    "            # Si no devuelve filas, solo confirmar la ejecución\n",
    "            return None\n",
    "\n",
    "    return result\n",
    "def eliminar_registros(tabla,usuario=usuario, password=password,host=host, database=basedatos):\n",
    "    \"\"\"\n",
    "    Elimina todos los registros de una tabla especificada en la base de datos.    \n",
    "    \"\"\"\n",
    "    # Crear el engine de SQLAlchemy\n",
    "    engine = create_engine(f'postgresql://{usuario}:{password}@{host}/{database}')\n",
    "    \n",
    "    with engine.connect() as connection:\n",
    "        # Ejecutar la sentencia SQL para eliminar todos los registros\n",
    "        connection.execute(text(f\"DELETE FROM {tabla};\"))\n",
    "        connection.commit()  # Confirmar los cambios\n",
    "\n",
    "def __actualizar_tabla_postgres(df:pd.DataFrame , tabla:str, columna_id:str , usuario=usuario, password=password, host= host, database=basedatos):\n",
    "    # Crear el engine de SQLAlchemy\n",
    "    engine = create_engine(f'postgresql://{usuario}:{password}@{host}/{database}')\n",
    "\n",
    "    # Usar pd.read_sql_query con una conexión\n",
    "        # Leer la tabla en un DataFrame de pandas\n",
    "    df_origen = ejecutar_query(f\"SELECT * FROM {tabla}\")\n",
    "\n",
    "    \n",
    "    # Renombrar la columna_id 'value' a 'name' si existe\n",
    "    if 'value' in df.columns:\n",
    "        df = df.rename(columns={'value': 'name'})\n",
    "    \n",
    "    # Si existe la columna_id uuid en la tabla original, crear esa columna_id\n",
    "    if 'uuid' in df_origen.columns:\n",
    "        df['uuid'] = [str(uuid.uuid4()) for _ in range(len(df))]\n",
    "    \n",
    "    if 'is_active' in df_origen.columns:\n",
    "        df['is_active'] = True\n",
    "\n",
    "\n",
    "    # Identificar las columna_ids que están en df pero no en df_origen\n",
    "    missing_columns = [col for col in df_origen.columns if col not in df.columns]\n",
    "\n",
    "    # Añadir las columna_ids faltantes a df con valores NaN\n",
    "    for col in missing_columns:\n",
    "        df[col] = None #pd.NA  # O usa otro valor predeterminado si es necesario\n",
    "\n",
    "    # Añadir las columna_ids comunes en las tablas\n",
    "    df['id'] = df.index\n",
    "    \n",
    "    df['created_by'] = 1\n",
    "    df['updated_by'] = 1\n",
    "    df['created_at'] = pd.Timestamp.now()\n",
    "    df['updated_at'] = pd.Timestamp.now()   \n",
    "\n",
    "    # Eliminar todos los registros de la tabla\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(f\"DELETE FROM {tabla};\"))\n",
    "        #connection.commit()\n",
    "    \n",
    "    # Insertar nuevos datos con pandas to_sql\n",
    "    df.to_sql(tabla, con= engine, if_exists='replace', index=False)\n",
    "\n",
    "    # Obtener el valor máximo de la columna_id 'id'\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(f\"SELECT MAX({columna_id}) FROM {tabla};\"))\n",
    "        max_id = result.scalar() or 0  # Si no hay registros, usar 0\n",
    "\n",
    "    \n",
    "    # Obtener el nombre de la secuencia asociada a la columna_id 'id'\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(f\"\"\"\n",
    "            SELECT pg_get_serial_sequence('{tabla}', '{columna_id}');\n",
    "        \"\"\"))\n",
    "        id_secuencia = result.scalar()  # Obtener el nombre de la secuencia\n",
    "    \n",
    "        \n",
    "    # Reiniciar el valor de la secuencia\n",
    "    with engine.connect() as connection:\n",
    "        # Si se tienen \n",
    "        if id_secuencia : connection.execute(text(f\"ALTER SEQUENCE {id_secuencia} RESTART WITH {max_id + 1};\"))\n",
    "\n",
    "#usuario='user_mantenimiento', password='pass_M4ntenimient0',host='192.168.100.56', database='db_mantenimiento_test'\n",
    "def update_plans_table(df, tabla, columna_id, usuario=usuario, password=password,host=host, database=basedatos ):\n",
    "    # Crear el engine de SQLAlchemy\n",
    "    engine = create_engine(f'postgresql://{usuario}:{password}@{host}/{database}')\n",
    "\n",
    "    # Leer la tabla en un DataFrame de pandas\n",
    "    df = pd.read_sql(f\"SELECT * FROM {tabla}\", engine)\n",
    "\n",
    "    # Renombrar columna_id 'value' a 'name' si existe\n",
    "    if 'value' in df.columns:\n",
    "        df = df.rename(columns={'value': 'name'})\n",
    "\n",
    "    # Identificar las columna_ids que están en df pero no en df\n",
    "    missing_columns = [col for col in df.columns if col not in df.columns]\n",
    "\n",
    "    # Añadir las columna_ids faltantes a df con valores NaN\n",
    "    for col in missing_columns:\n",
    "        df[col] = None  # O usa otro valor predeterminado si es necesario\n",
    "\n",
    "    # Añadir/actualizar las columna_ids necesarias en df\n",
    "    df['id'] = df.index\n",
    "    df['is_active'] = True\n",
    "    df['created_by'] = 1\n",
    "    df['updated_by'] = 1\n",
    "    df['created_at'] = pd.Timestamp.now()\n",
    "    df['updated_at'] = pd.Timestamp.now()\n",
    "\n",
    "    # Obtener el nombre de la secuencia asociada a la columna_id 'id' en la tabla 'plans_test'\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(f\"SELECT pg_get_serial_sequence('{tabla}', '{columna_id}');\"))\n",
    "        id_secuencia = result.scalar()  # Obtener el nombre de la secuencia\n",
    "\n",
    "    # Eliminar todos los registros de la tabla TODO: ANALIZAR LA ELIMINACION DE LOS DATOS DE LA TABLA\n",
    "    #with engine.connect() as connection:\n",
    "    #    connection.execute(text(f\"DELETE FROM {tabla};\"))\n",
    "    #    connection.commit()\n",
    "\n",
    "    # Insertar nuevos datos con pandas to_sql\n",
    "    df.to_sql(tabla, engine, if_exists='replace', index=False)\n",
    "\n",
    "    # Obtener el valor máximo de la columna_id 'id'\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(f\"SELECT MAX(id) FROM {tabla};\"))\n",
    "        max_id = result.scalar() or 0  # Si no hay registros, usar 0\n",
    "\n",
    "    # Reiniciar el valor de la secuencia\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(f\"ALTER SEQUENCE {id_secuencia} RESTART WITH {max_id + 1};\"))\n",
    "\n",
    "# Ejemplo de uso\n",
    "# Definir el DataFrame df con tus datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRIPT FASE 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "(psycopg2.OperationalError) connection to server at \"192.168.100.56\", port 5432 failed: FATAL:  no pg_hba.conf entry for host \"192.168.11.41\", user \"user_gom_mantenimiento\", database \"db_mantenimiento\", SSL encryption\nconnection to server at \"192.168.100.56\", port 5432 failed: FATAL:  no pg_hba.conf entry for host \"192.168.11.41\", user \"user_gom_mantenimiento\", database \"db_mantenimiento\", no encryption\n\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:146\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[1;34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dbapi_connection \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39;49mraw_connection()\n\u001b[0;32m    147\u001b[0m \u001b[39mexcept\u001b[39;00m dialect\u001b[39m.\u001b[39mloaded_dbapi\u001b[39m.\u001b[39mError \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3298\u001b[0m, in \u001b[0;36mEngine.raw_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3277\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[0;32m   3278\u001b[0m \n\u001b[0;32m   3279\u001b[0m \u001b[39mThe returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3296\u001b[0m \n\u001b[0;32m   3297\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 3298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpool\u001b[39m.\u001b[39;49mconnect()\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:449\u001b[0m, in \u001b[0;36mPool.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \n\u001b[0;32m    444\u001b[0m \u001b[39mThe connection is instrumented such that when its\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    447\u001b[0m \n\u001b[0;32m    448\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m \u001b[39mreturn\u001b[39;00m _ConnectionFairy\u001b[39m.\u001b[39;49m_checkout(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:1263\u001b[0m, in \u001b[0;36m_ConnectionFairy._checkout\u001b[1;34m(cls, pool, threadconns, fairy)\u001b[0m\n\u001b[0;32m   1262\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fairy:\n\u001b[1;32m-> 1263\u001b[0m     fairy \u001b[39m=\u001b[39m _ConnectionRecord\u001b[39m.\u001b[39;49mcheckout(pool)\n\u001b[0;32m   1265\u001b[0m     \u001b[39mif\u001b[39;00m threadconns \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:712\u001b[0m, in \u001b[0;36m_ConnectionRecord.checkout\u001b[1;34m(cls, pool)\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 712\u001b[0m     rec \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49m_do_get()\n\u001b[0;32m    714\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:179\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[39mwith\u001b[39;00m util\u001b[39m.\u001b[39;49msafe_reraise():\n\u001b[0;32m    180\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dec_overflow()\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:146\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# remove potential circular references\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m     \u001b[39mraise\u001b[39;00m exc_value\u001b[39m.\u001b[39mwith_traceback(exc_tb)\n\u001b[0;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:177\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_connection()\n\u001b[0;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:390\u001b[0m, in \u001b[0;36mPool._create_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 390\u001b[0m \u001b[39mreturn\u001b[39;00m _ConnectionRecord(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:674\u001b[0m, in \u001b[0;36m_ConnectionRecord.__init__\u001b[1;34m(self, pool, connect)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[39mif\u001b[39;00m connect:\n\u001b[1;32m--> 674\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__connect()\n\u001b[0;32m    675\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalize_callback \u001b[39m=\u001b[39m deque()\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:900\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 900\u001b[0m     \u001b[39mwith\u001b[39;00m util\u001b[39m.\u001b[39;49msafe_reraise():\n\u001b[0;32m    901\u001b[0m         pool\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mError on connect(): \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, e)\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:146\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# remove potential circular references\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m     \u001b[39mraise\u001b[39;00m exc_value\u001b[39m.\u001b[39mwith_traceback(exc_tb)\n\u001b[0;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:896\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    895\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstarttime \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> 896\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdbapi_connection \u001b[39m=\u001b[39m connection \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49m_invoke_creator(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    897\u001b[0m pool\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mCreated new connection \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, connection)\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\create.py:646\u001b[0m, in \u001b[0;36mcreate_engine.<locals>.connect\u001b[1;34m(connection_record)\u001b[0m\n\u001b[0;32m    644\u001b[0m             \u001b[39mreturn\u001b[39;00m connection\n\u001b[1;32m--> 646\u001b[0m \u001b[39mreturn\u001b[39;00m dialect\u001b[39m.\u001b[39;49mconnect(\u001b[39m*\u001b[39;49mcargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcparams)\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:622\u001b[0m, in \u001b[0;36mDefaultDialect.connect\u001b[1;34m(self, *cargs, **cparams)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mcargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcparams):\n\u001b[0;32m    621\u001b[0m     \u001b[39m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[1;32m--> 622\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloaded_dbapi\u001b[39m.\u001b[39;49mconnect(\u001b[39m*\u001b[39;49mcargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcparams)\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\psycopg2\\__init__.py:135\u001b[0m, in \u001b[0;36mconnect\u001b[1;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m dsn \u001b[39m=\u001b[39m _ext\u001b[39m.\u001b[39mmake_dsn(dsn, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 135\u001b[0m conn \u001b[39m=\u001b[39m _connect(dsn, connection_factory\u001b[39m=\u001b[39;49mconnection_factory, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwasync)\n\u001b[0;32m    136\u001b[0m \u001b[39mif\u001b[39;00m cursor_factory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mOperationalError\u001b[0m: connection to server at \"192.168.100.56\", port 5432 failed: FATAL:  no pg_hba.conf entry for host \"192.168.11.41\", user \"user_gom_mantenimiento\", database \"db_mantenimiento\", SSL encryption\nconnection to server at \"192.168.100.56\", port 5432 failed: FATAL:  no pg_hba.conf entry for host \"192.168.11.41\", user \"user_gom_mantenimiento\", database \"db_mantenimiento\", no encryption\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 17\u001b[0m\n\u001b[0;32m      1\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[39mSELECT\t\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[39m\tb.id AS fk_base,\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39m  b.id ASC\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m---> 17\u001b[0m df_base_activities \u001b[39m=\u001b[39m  ejecutar_query(query)\n\u001b[0;32m     18\u001b[0m df_base_activities\n\u001b[0;32m     21\u001b[0m \u001b[39m#obtener_registros('base')\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 127\u001b[0m, in \u001b[0;36mejecutar_query\u001b[1;34m(query, usuario, password, host, database)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[39m# Crear el engine de SQLAlchemy\u001b[39;00m\n\u001b[0;32m    125\u001b[0m engine \u001b[39m=\u001b[39m create_engine(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpostgresql://\u001b[39m\u001b[39m{\u001b[39;00musuario\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m{\u001b[39;00mpassword\u001b[39m}\u001b[39;00m\u001b[39m@\u001b[39m\u001b[39m{\u001b[39;00mhost\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mdatabase\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 127\u001b[0m \u001b[39mwith\u001b[39;00m engine\u001b[39m.\u001b[39;49mconnect() \u001b[39mas\u001b[39;00m connection:\n\u001b[0;32m    128\u001b[0m     \u001b[39m# Ejecutar la consulta\u001b[39;00m\n\u001b[0;32m    129\u001b[0m     result \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39mexecute(text(query))\n\u001b[0;32m    130\u001b[0m     connection\u001b[39m.\u001b[39mcommit()\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3274\u001b[0m, in \u001b[0;36mEngine.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3251\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Connection:\n\u001b[0;32m   3252\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a new :class:`_engine.Connection` object.\u001b[39;00m\n\u001b[0;32m   3253\u001b[0m \n\u001b[0;32m   3254\u001b[0m \u001b[39m    The :class:`_engine.Connection` acts as a Python context manager, so\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3271\u001b[0m \n\u001b[0;32m   3272\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connection_cls(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:148\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[1;34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dbapi_connection \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39mraw_connection()\n\u001b[0;32m    147\u001b[0m     \u001b[39mexcept\u001b[39;00m dialect\u001b[39m.\u001b[39mloaded_dbapi\u001b[39m.\u001b[39mError \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 148\u001b[0m         Connection\u001b[39m.\u001b[39;49m_handle_dbapi_exception_noconnection(\n\u001b[0;32m    149\u001b[0m             err, dialect, engine\n\u001b[0;32m    150\u001b[0m         )\n\u001b[0;32m    151\u001b[0m         \u001b[39mraise\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:2439\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception_noconnection\u001b[1;34m(cls, e, dialect, engine, is_disconnect, invalidate_pool_on_disconnect, is_pre_ping)\u001b[0m\n\u001b[0;32m   2437\u001b[0m \u001b[39melif\u001b[39;00m should_wrap:\n\u001b[0;32m   2438\u001b[0m     \u001b[39massert\u001b[39;00m sqlalchemy_exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2439\u001b[0m     \u001b[39mraise\u001b[39;00m sqlalchemy_exception\u001b[39m.\u001b[39mwith_traceback(exc_info[\u001b[39m2\u001b[39m]) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39me\u001b[39;00m\n\u001b[0;32m   2440\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2441\u001b[0m     \u001b[39massert\u001b[39;00m exc_info[\u001b[39m1\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:146\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[1;34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dbapi_connection \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39;49mraw_connection()\n\u001b[0;32m    147\u001b[0m     \u001b[39mexcept\u001b[39;00m dialect\u001b[39m.\u001b[39mloaded_dbapi\u001b[39m.\u001b[39mError \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    148\u001b[0m         Connection\u001b[39m.\u001b[39m_handle_dbapi_exception_noconnection(\n\u001b[0;32m    149\u001b[0m             err, dialect, engine\n\u001b[0;32m    150\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3298\u001b[0m, in \u001b[0;36mEngine.raw_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3276\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mraw_connection\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PoolProxiedConnection:\n\u001b[0;32m   3277\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[0;32m   3278\u001b[0m \n\u001b[0;32m   3279\u001b[0m \u001b[39m    The returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3296\u001b[0m \n\u001b[0;32m   3297\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3298\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpool\u001b[39m.\u001b[39;49mconnect()\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:449\u001b[0m, in \u001b[0;36mPool.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PoolProxiedConnection:\n\u001b[0;32m    442\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \n\u001b[0;32m    444\u001b[0m \u001b[39m    The connection is instrumented such that when its\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    447\u001b[0m \n\u001b[0;32m    448\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m     \u001b[39mreturn\u001b[39;00m _ConnectionFairy\u001b[39m.\u001b[39;49m_checkout(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:1263\u001b[0m, in \u001b[0;36m_ConnectionFairy._checkout\u001b[1;34m(cls, pool, threadconns, fairy)\u001b[0m\n\u001b[0;32m   1255\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m   1256\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_checkout\u001b[39m(\n\u001b[0;32m   1257\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     fairy: Optional[_ConnectionFairy] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1261\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m _ConnectionFairy:\n\u001b[0;32m   1262\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fairy:\n\u001b[1;32m-> 1263\u001b[0m         fairy \u001b[39m=\u001b[39m _ConnectionRecord\u001b[39m.\u001b[39;49mcheckout(pool)\n\u001b[0;32m   1265\u001b[0m         \u001b[39mif\u001b[39;00m threadconns \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1266\u001b[0m             threadconns\u001b[39m.\u001b[39mcurrent \u001b[39m=\u001b[39m weakref\u001b[39m.\u001b[39mref(fairy)\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:712\u001b[0m, in \u001b[0;36m_ConnectionRecord.checkout\u001b[1;34m(cls, pool)\u001b[0m\n\u001b[0;32m    710\u001b[0m     rec \u001b[39m=\u001b[39m cast(_ConnectionRecord, pool\u001b[39m.\u001b[39m_do_get())\n\u001b[0;32m    711\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 712\u001b[0m     rec \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49m_do_get()\n\u001b[0;32m    714\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    715\u001b[0m     dbapi_connection \u001b[39m=\u001b[39m rec\u001b[39m.\u001b[39mget_connection()\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:179\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection()\n\u001b[0;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[39mwith\u001b[39;00m util\u001b[39m.\u001b[39;49msafe_reraise():\n\u001b[0;32m    180\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dec_overflow()\n\u001b[0;32m    181\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:146\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[39massert\u001b[39;00m exc_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# remove potential circular references\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m     \u001b[39mraise\u001b[39;00m exc_value\u001b[39m.\u001b[39mwith_traceback(exc_tb)\n\u001b[0;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# remove potential circular references\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:177\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inc_overflow():\n\u001b[0;32m    176\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_connection()\n\u001b[0;32m    178\u001b[0m     \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m         \u001b[39mwith\u001b[39;00m util\u001b[39m.\u001b[39msafe_reraise():\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:390\u001b[0m, in \u001b[0;36mPool._create_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_create_connection\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ConnectionPoolEntry:\n\u001b[0;32m    388\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 390\u001b[0m     \u001b[39mreturn\u001b[39;00m _ConnectionRecord(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:674\u001b[0m, in \u001b[0;36m_ConnectionRecord.__init__\u001b[1;34m(self, pool, connect)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__pool \u001b[39m=\u001b[39m pool\n\u001b[0;32m    673\u001b[0m \u001b[39mif\u001b[39;00m connect:\n\u001b[1;32m--> 674\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__connect()\n\u001b[0;32m    675\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalize_callback \u001b[39m=\u001b[39m deque()\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:900\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfresh \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 900\u001b[0m     \u001b[39mwith\u001b[39;00m util\u001b[39m.\u001b[39;49msafe_reraise():\n\u001b[0;32m    901\u001b[0m         pool\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mError on connect(): \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m    902\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    903\u001b[0m     \u001b[39m# in SQLAlchemy 1.4 the first_connect event is not used by\u001b[39;00m\n\u001b[0;32m    904\u001b[0m     \u001b[39m# the engine, so this will usually not be set\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:146\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[39massert\u001b[39;00m exc_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# remove potential circular references\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m     \u001b[39mraise\u001b[39;00m exc_value\u001b[39m.\u001b[39mwith_traceback(exc_tb)\n\u001b[0;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# remove potential circular references\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:896\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    895\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstarttime \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> 896\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdbapi_connection \u001b[39m=\u001b[39m connection \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49m_invoke_creator(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    897\u001b[0m     pool\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mCreated new connection \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, connection)\n\u001b[0;32m    898\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfresh \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\create.py:646\u001b[0m, in \u001b[0;36mcreate_engine.<locals>.connect\u001b[1;34m(connection_record)\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    644\u001b[0m             \u001b[39mreturn\u001b[39;00m connection\n\u001b[1;32m--> 646\u001b[0m \u001b[39mreturn\u001b[39;00m dialect\u001b[39m.\u001b[39;49mconnect(\u001b[39m*\u001b[39;49mcargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcparams)\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:622\u001b[0m, in \u001b[0;36mDefaultDialect.connect\u001b[1;34m(self, *cargs, **cparams)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mcargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcparams):\n\u001b[0;32m    621\u001b[0m     \u001b[39m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[1;32m--> 622\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloaded_dbapi\u001b[39m.\u001b[39;49mconnect(\u001b[39m*\u001b[39;49mcargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcparams)\n",
      "File \u001b[1;32mc:\\Users\\ramamani\\Proyectos\\datascience_etcmt\\.venv\\Lib\\site-packages\\psycopg2\\__init__.py:135\u001b[0m, in \u001b[0;36mconnect\u001b[1;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m     kwasync[\u001b[39m'\u001b[39m\u001b[39masync_\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39masync_\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    134\u001b[0m dsn \u001b[39m=\u001b[39m _ext\u001b[39m.\u001b[39mmake_dsn(dsn, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 135\u001b[0m conn \u001b[39m=\u001b[39m _connect(dsn, connection_factory\u001b[39m=\u001b[39;49mconnection_factory, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwasync)\n\u001b[0;32m    136\u001b[0m \u001b[39mif\u001b[39;00m cursor_factory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     conn\u001b[39m.\u001b[39mcursor_factory \u001b[39m=\u001b[39m cursor_factory\n",
      "\u001b[1;31mOperationalError\u001b[0m: (psycopg2.OperationalError) connection to server at \"192.168.100.56\", port 5432 failed: FATAL:  no pg_hba.conf entry for host \"192.168.11.41\", user \"user_gom_mantenimiento\", database \"db_mantenimiento\", SSL encryption\nconnection to server at \"192.168.100.56\", port 5432 failed: FATAL:  no pg_hba.conf entry for host \"192.168.11.41\", user \"user_gom_mantenimiento\", database \"db_mantenimiento\", no encryption\n\n(Background on this error at: https://sqlalche.me/e/20/e3q8)"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\t\n",
    "\tb.id AS fk_base,\n",
    "  --p.id AS id_plan, \n",
    "\ta.id AS fk_activity\n",
    "FROM\n",
    "\tbase AS b LEFT JOIN\tplans AS p\tON b.fk_plan = p.id\n",
    "\tLEFT JOIN\tactivities AS a ON p.id = a.fk_plan\n",
    "WHERE\n",
    "\tb.fk_plan IS NOT NULL\n",
    "ORDER BY\n",
    "\tb.fk_plan ASC,\n",
    "  a.id ASC,\n",
    "  b.id ASC\n",
    "\"\"\"\n",
    "\n",
    "df_base_activities =  ejecutar_query(query)\n",
    "df_base_activities\n",
    "\n",
    "\n",
    "#obtener_registros('base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>uuid</th>\n",
       "      <th>fk_base</th>\n",
       "      <th>fk_activity</th>\n",
       "      <th>start_date</th>\n",
       "      <th>last_date</th>\n",
       "      <th>next_date</th>\n",
       "      <th>rescheduled_date</th>\n",
       "      <th>start_reading</th>\n",
       "      <th>last_reading</th>\n",
       "      <th>next_reading</th>\n",
       "      <th>rescheduled_reading</th>\n",
       "      <th>created_at</th>\n",
       "      <th>created_by</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>updated_by</th>\n",
       "      <th>deleted_at</th>\n",
       "      <th>deleted_by</th>\n",
       "      <th>fk_base_metric</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, uuid, fk_base, fk_activity, start_date, last_date, next_date, rescheduled_date, start_reading, last_reading, next_reading, rescheduled_reading, created_at, created_by, updated_at, updated_by, deleted_at, deleted_by, fk_base_metric]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#obtener_registros('base_activities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_df_base_activities = format_dataframe(df_base_activities,'base_activities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eliminar_registros('base_activities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar registros con truncate de la tabla base_activities\n",
    "ejecutar_query('TRUNCATE base_activities CASCADE;')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se reinició el índice public.base_activities_id_seq en 88316\n"
     ]
    }
   ],
   "source": [
    "actualizar_tabla_postgres(format_df_base_activities,'base_activities','id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se requiere que las actividades, sean inicializadas, si la actividad es por fecha, se requiere que se inicialicen en una fecha determinada, esta fecha debera obtenerse posteriormente de una hoja de calculo donde los responsables deberan registrar la fecha en la que se ejecuto por ultima vez, para que realice el calculo de la proxima intervencion\n",
    "# En el caso de start_date, colocar como referencia la fecha de inicio de operacion de la linea TODO: Se requiere que se tome en cuenta en hoja de calculo las fechas de inicio de operacion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasos\n",
    "1. Eliminar registros de activities\n",
    "2. Eliminar de tabla base que este relacionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Eliminar datos tabla actividades\n",
    "eliminar_registros(tabla='activities')\n",
    "# Eliminar registros de la tabla base que tenga items relacionados\n",
    "query = 'delete from base where fk_plan is not null '\n",
    "ejecutar_query(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MOTOR ELECTRICO PRINCIPAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REFRIGERACION DE MOTOR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       value\n",
       "1  MOTOR ELECTRICO PRINCIPAL\n",
       "2     REFRIGERACION DE MOTOR"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_plan.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install SQLAlchemy psycopg2-binary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabla actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se reinició el índice public.actions_id_seq en 16\n"
     ]
    }
   ],
   "source": [
    "# Tabla actions, darle formato y actualizar en BBDD\n",
    "df_action =format_dataframe(df_action,'actions')\n",
    "actualizar_tabla_postgres(df_action,'actions','id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabla Periodicities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>created_at</th>\n",
       "      <th>created_by</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>updated_by</th>\n",
       "      <th>deleted_at</th>\n",
       "      <th>deleted_by</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>dia(s)</td>\n",
       "      <td>2022-05-30 12:05:19.768000-04:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-05-30 12:05:19.768000-04:00</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>semana(s)</td>\n",
       "      <td>2022-05-30 12:05:19.768000-04:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-05-30 12:05:19.768000-04:00</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>mes(es)</td>\n",
       "      <td>2022-05-30 12:05:19.768000-04:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-05-30 12:05:19.768000-04:00</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>año(s)</td>\n",
       "      <td>2022-05-30 12:05:19.768000-04:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-05-30 12:05:19.768000-04:00</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>lunes</td>\n",
       "      <td>2022-05-30 12:05:19.768000-04:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-05-30 12:05:19.768000-04:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-12-17 08:55:19.768000-04:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>martes</td>\n",
       "      <td>2022-05-30 12:05:19.768000-04:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-05-30 12:05:19.768000-04:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-12-17 08:55:19.768000-04:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>miercoles</td>\n",
       "      <td>2022-05-30 12:05:19.768000-04:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-05-30 12:05:19.768000-04:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-12-17 08:55:19.768000-04:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>jueves</td>\n",
       "      <td>2022-05-30 12:05:19.768000-04:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-05-30 12:05:19.768000-04:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-12-17 08:55:19.768000-04:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>viernes</td>\n",
       "      <td>2022-05-30 12:05:19.768000-04:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-05-30 12:05:19.768000-04:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-12-17 08:55:19.768000-04:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>sábado</td>\n",
       "      <td>2022-05-30 12:05:19.768000-04:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-05-30 12:05:19.768000-04:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-12-17 08:55:19.768000-04:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>domingo</td>\n",
       "      <td>2022-05-30 12:05:19.768000-04:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-05-30 12:05:19.768000-04:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-12-17 08:55:19.768000-04:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id       name                       created_at  created_by  \\\n",
       "id                                                               \n",
       "1    1     dia(s) 2022-05-30 12:05:19.768000-04:00           1   \n",
       "2    2  semana(s) 2022-05-30 12:05:19.768000-04:00           1   \n",
       "3    3    mes(es) 2022-05-30 12:05:19.768000-04:00           1   \n",
       "4    4     año(s) 2022-05-30 12:05:19.768000-04:00           1   \n",
       "5    5      lunes 2022-05-30 12:05:19.768000-04:00           1   \n",
       "6    6     martes 2022-05-30 12:05:19.768000-04:00           1   \n",
       "7    7  miercoles 2022-05-30 12:05:19.768000-04:00           1   \n",
       "8    8     jueves 2022-05-30 12:05:19.768000-04:00           1   \n",
       "9    9    viernes 2022-05-30 12:05:19.768000-04:00           1   \n",
       "10  10     sábado 2022-05-30 12:05:19.768000-04:00           1   \n",
       "11  11    domingo 2022-05-30 12:05:19.768000-04:00           1   \n",
       "\n",
       "                         updated_at  updated_by  \\\n",
       "id                                                \n",
       "1  2022-05-30 12:05:19.768000-04:00           1   \n",
       "2  2022-05-30 12:05:19.768000-04:00           1   \n",
       "3  2022-05-30 12:05:19.768000-04:00           1   \n",
       "4  2022-05-30 12:05:19.768000-04:00           1   \n",
       "5  2022-05-30 12:05:19.768000-04:00           1   \n",
       "6  2022-05-30 12:05:19.768000-04:00           1   \n",
       "7  2022-05-30 12:05:19.768000-04:00           1   \n",
       "8  2022-05-30 12:05:19.768000-04:00           1   \n",
       "9  2022-05-30 12:05:19.768000-04:00           1   \n",
       "10 2022-05-30 12:05:19.768000-04:00           1   \n",
       "11 2022-05-30 12:05:19.768000-04:00           1   \n",
       "\n",
       "                         deleted_at  deleted_by  \n",
       "id                                               \n",
       "1                               NaT         NaN  \n",
       "2                               NaT         NaN  \n",
       "3                               NaT         NaN  \n",
       "4                               NaT         NaN  \n",
       "5  2024-12-17 08:55:19.768000-04:00         1.0  \n",
       "6  2024-12-17 08:55:19.768000-04:00         1.0  \n",
       "7  2024-12-17 08:55:19.768000-04:00         1.0  \n",
       "8  2024-12-17 08:55:19.768000-04:00         1.0  \n",
       "9  2024-12-17 08:55:19.768000-04:00         1.0  \n",
       "10 2024-12-17 08:55:19.768000-04:00         1.0  \n",
       "11 2024-12-17 08:55:19.768000-04:00         1.0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_periodicities = obtener_registros(tabla='periodicities')\n",
    "# Se cambio la periodicidad\n",
    "df_periodicities = obtener_registros(tabla='periodicity_unit')\n",
    "df_periodicities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## specialties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se reinició el índice public.specialties_id_seq en 9\n"
     ]
    }
   ],
   "source": [
    "df_speciality = format_dataframe(df_speciality,'specialties')\n",
    "df_speciality['description'] = df_speciality['name'] # no se crea automaticamente\n",
    "actualizar_tabla_postgres(df=df_speciality,tabla='specialties',columna_id='id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se reinició el índice public.plans_id_seq en 279\n"
     ]
    }
   ],
   "source": [
    "# Tabla actions, darle formato y actualizar en BBDD\n",
    "df_plan = format_dataframe(df_plan,'plans')\n",
    "actualizar_tabla_postgres(df_plan,'plans','id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usage_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se reinició el índice public.usage_units_id_seq en 3\n"
     ]
    }
   ],
   "source": [
    "df_usage_units = pd.DataFrame(df_activities['fk_usage_unit'].dropna().unique(),columns=['value'])\n",
    "df_usage_units = format_dataframe(df_usage_units,'usage_units')\n",
    "df_usage_units['unit']=df_usage_units['name']           #TODO: ???\n",
    "df_usage_units['description']=df_usage_units['name']    #TODO: ???\n",
    "actualizar_tabla_postgres(df_usage_units,'usage_units',columna_id='id')\n",
    "df_usage_units.index = df_usage_units.id    # Se coloca el id correspondientea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>created_by</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>updated_by</th>\n",
       "      <th>deleted_at</th>\n",
       "      <th>deleted_by</th>\n",
       "      <th>unit</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>horas</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-01-21 11:46:45.455170</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-01-21 11:46:45.455403</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>horas</td>\n",
       "      <td>horas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ciclos</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-01-21 11:46:45.455170</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-01-21 11:46:45.455403</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ciclos</td>\n",
       "      <td>ciclos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name  id                 created_at  created_by  \\\n",
       "id                                                      \n",
       "1    horas   1 2025-01-21 11:46:45.455170           1   \n",
       "2   ciclos   2 2025-01-21 11:46:45.455170           1   \n",
       "\n",
       "                   updated_at  updated_by deleted_at deleted_by    unit  \\\n",
       "id                                                                        \n",
       "1  2025-01-21 11:46:45.455403           1       None       None   horas   \n",
       "2  2025-01-21 11:46:45.455403           1       None       None  ciclos   \n",
       "\n",
       "   description  \n",
       "id              \n",
       "1        horas  \n",
       "2       ciclos  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_usage_units"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fk_classifiers_type</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>grup_1</th>\n",
       "      <th>notEnabled</th>\n",
       "      <th>is_active</th>\n",
       "      <th>created_by</th>\n",
       "      <th>updated_by</th>\n",
       "      <th>deleted_by</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>deleted_at</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>ACORDE A LA DISPONIBILIDAD</td>\n",
       "      <td>ACORDE A LA DISPONIBILIDAD</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-05-30 12:05:18.891000-04:00</td>\n",
       "      <td>2022-05-30 12:05:18.891000-04:00</td>\n",
       "      <td>2022-07-18 10:05:18.891000-04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>ELECTRICO</td>\n",
       "      <td>ELECTRICO</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-05-30 12:05:18.891000-04:00</td>\n",
       "      <td>2022-05-30 12:05:18.891000-04:00</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  fk_classifiers_type                        name  \\\n",
       "id                                                        \n",
       "5    5                    3  ACORDE A LA DISPONIBILIDAD   \n",
       "17  17                    5                   ELECTRICO   \n",
       "\n",
       "                   description  grup_1 notEnabled  is_active  created_by  \\\n",
       "id                                                                         \n",
       "5   ACORDE A LA DISPONIBILIDAD       1       None      False           1   \n",
       "17                   ELECTRICO       1       None       True           1   \n",
       "\n",
       "    updated_by  deleted_by                       created_at  \\\n",
       "id                                                            \n",
       "5          1.0         1.0 2022-05-30 12:05:18.891000-04:00   \n",
       "17         1.0         NaN 2022-05-30 12:05:18.891000-04:00   \n",
       "\n",
       "                         updated_at                       deleted_at  \n",
       "id                                                                    \n",
       "5  2022-05-30 12:05:18.891000-04:00 2022-07-18 10:05:18.891000-04:00  \n",
       "17 2022-05-30 12:05:18.891000-04:00                              NaT  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtener registros de la tabla classifiers\n",
    "df_classifiers = obtener_registros(tabla='classifiers')\n",
    "df_classifiers.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dar formato a tabla\n",
    "df_activities = format_dataframe(df_activities,'activities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer copia local\n",
    "df_activities_raw = df_activities.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fk_activity</th>\n",
       "      <th>fk_plan</th>\n",
       "      <th>fk_action</th>\n",
       "      <th>name</th>\n",
       "      <th>fkc_activity_type</th>\n",
       "      <th>fkc_priority</th>\n",
       "      <th>fk_specialty</th>\n",
       "      <th>fkc_regime</th>\n",
       "      <th>stoppage</th>\n",
       "      <th>time_interval_value</th>\n",
       "      <th>...</th>\n",
       "      <th>created_by</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>updated_by</th>\n",
       "      <th>deleted_at</th>\n",
       "      <th>deleted_by</th>\n",
       "      <th>earliest_reschedule_days</th>\n",
       "      <th>latest_reschedule_days</th>\n",
       "      <th>earliest_reschedule_usage</th>\n",
       "      <th>latest_reschedule_usage</th>\n",
       "      <th>skippable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Verificar la ausencia de ruidos y vibraciones ...</td>\n",
       "      <td>Actividad</td>\n",
       "      <td>MEDIA</td>\n",
       "      <td>1</td>\n",
       "      <td>FECHAS</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-01-21 11:47:06.243646</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Verificación de marcas de tornillería del moto...</td>\n",
       "      <td>Actividad</td>\n",
       "      <td>MEDIA</td>\n",
       "      <td>1</td>\n",
       "      <td>FECHAS</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-01-21 11:47:06.243646</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  fk_activity  fk_plan  fk_action  \\\n",
       "1        None        1          1   \n",
       "2        None        1          1   \n",
       "\n",
       "                                                name fkc_activity_type  \\\n",
       "1  Verificar la ausencia de ruidos y vibraciones ...         Actividad   \n",
       "2  Verificación de marcas de tornillería del moto...         Actividad   \n",
       "\n",
       "  fkc_priority  fk_specialty fkc_regime stoppage  time_interval_value  ...  \\\n",
       "1        MEDIA             1     FECHAS    False                  1.0  ...   \n",
       "2        MEDIA             1     FECHAS    False                  1.0  ...   \n",
       "\n",
       "  created_by                 updated_at updated_by deleted_at  deleted_by  \\\n",
       "1          1 2025-01-21 11:47:06.243646          1       None        None   \n",
       "2          1 2025-01-21 11:47:06.243646          1       None        None   \n",
       "\n",
       "   earliest_reschedule_days latest_reschedule_days  earliest_reschedule_usage  \\\n",
       "1                      None                   None                       None   \n",
       "2                      None                   None                       None   \n",
       "\n",
       "  latest_reschedule_usage  skippable  \n",
       "1                    None       None  \n",
       "2                    None       None  \n",
       "\n",
       "[2 rows x 27 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_activities.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fk_activity</th>\n",
       "      <th>fk_plan</th>\n",
       "      <th>fk_action</th>\n",
       "      <th>name</th>\n",
       "      <th>fkc_activity_type</th>\n",
       "      <th>fkc_priority</th>\n",
       "      <th>fk_specialty</th>\n",
       "      <th>fkc_regime</th>\n",
       "      <th>stoppage</th>\n",
       "      <th>time_interval_value</th>\n",
       "      <th>...</th>\n",
       "      <th>created_by</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>updated_by</th>\n",
       "      <th>deleted_at</th>\n",
       "      <th>deleted_by</th>\n",
       "      <th>earliest_reschedule_days</th>\n",
       "      <th>latest_reschedule_days</th>\n",
       "      <th>earliest_reschedule_usage</th>\n",
       "      <th>latest_reschedule_usage</th>\n",
       "      <th>skippable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Verificar la ausencia de ruidos y vibraciones ...</td>\n",
       "      <td>166</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-01-21 11:47:06.243646</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Verificación de marcas de tornillería del moto...</td>\n",
       "      <td>166</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-01-21 11:47:06.243646</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  fk_activity  fk_plan  fk_action  \\\n",
       "1        None        1          1   \n",
       "2        None        1          1   \n",
       "\n",
       "                                                name  fkc_activity_type  \\\n",
       "1  Verificar la ausencia de ruidos y vibraciones ...                166   \n",
       "2  Verificación de marcas de tornillería del moto...                166   \n",
       "\n",
       "   fkc_priority  fk_specialty  fkc_regime stoppage  time_interval_value  ...  \\\n",
       "1             8             1         163    False                  1.0  ...   \n",
       "2             8             1         163    False                  1.0  ...   \n",
       "\n",
       "  created_by                 updated_at updated_by deleted_at  deleted_by  \\\n",
       "1          1 2025-01-21 11:47:06.243646          1       None        None   \n",
       "2          1 2025-01-21 11:47:06.243646          1       None        None   \n",
       "\n",
       "   earliest_reschedule_days latest_reschedule_days  earliest_reschedule_usage  \\\n",
       "1                      None                   None                       None   \n",
       "2                      None                   None                       None   \n",
       "\n",
       "  latest_reschedule_usage  skippable  \n",
       "1                    None       None  \n",
       "2                    None       None  \n",
       "\n",
       "[2 rows x 27 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertir en sus indices todas las columnas que tengan que ver con classifiers\n",
    "df_activities['fkc_activity_type'] =df_activities['fkc_activity_type'].apply(lambda x: gs1.buscarIndice(valor=str(x),df=df_classifiers,columna_id='name')) \n",
    "df_activities['fkc_priority'] = df_activities['fkc_priority'].apply(lambda x: gs1.buscarIndice(df=df_classifiers,valor=str(x),columna_id='name'))\n",
    "df_activities['fkc_regime'] = df_activities['fkc_regime'].apply(lambda x: gs1.buscarIndice(df=df_classifiers,valor=str(x),columna_id='name'))\n",
    "\n",
    "#Obtener periodiciadad unit\n",
    "df_activities['fk_periodicity_unit'] = df_activities['fk_periodicity_unit'].apply(lambda x: gs1.buscarIndice(valor=str(x),df=df_periodicities,columna_id='name'))\n",
    "# obtener usage_unit\n",
    "df_activities['fk_usage_unit'] = df_activities['fk_usage_unit'].apply(lambda x: gs1.buscarIndice(valor=str(x),df=df_usage_units,columna_id='name'))\n",
    "\n",
    "# en la columna fk_periodicity_unit tienen valores 'NONE', necesito que nos valores 'NONE' se conviertan en valores vacios\n",
    "df_activities['fk_periodicity_unit'] = df_activities['fk_periodicity_unit'].replace('NONE', None)\n",
    "\n",
    "# en la columna fk_usage_unit tiene valores \"NAN\", reemplazar valores NAN con valores vacios o None\n",
    "df_activities['fk_usage_unit'] = df_activities['fk_usage_unit'].replace('NAN', None)\n",
    "\n",
    "df_activities.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_activities.to_excel(\"activities_antes_bd.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se reinició el índice public.activities_id_seq en 2881\n"
     ]
    }
   ],
   "source": [
    "actualizar_tabla_postgres(df_activities,'activities',columna_id='id')\n",
    "\n",
    "#InvalidTextRepresentation: la sintaxis de entrada no es válida para tipo integer: «nan»\n",
    "#LINE 1: ...l motor.', 166, 8, 1, 163, false, 1.0, '1', NULL, 'nan', 'd3...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completar unidades medida y de uso\n",
    "\n",
    "De df_activities de la columna fk_periodicity_unit, si el valor de la columna es ['horas','ciclos'] mover el contenido de la columna time_interval_value a la columna usage_interval_value en la columna time_interval_value dejar con valor nulo, y mover el valor de la columna fk_peridicity_unit  a la columna fk_usage_unit y en la columna fk_periodicity_unit dejar en nulo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install sqlalchemy==1.4.23 psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalación\n",
    "Se colocaron todos los paquetes que se utilizan en el proyecto en el archivo paquetes.txt\n",
    "En caso que no funcione utilizar:\n",
    "\n",
    "<code> pip install SQLAlchemy psycopg2-binary </code>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install SQLAlchemy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportar a archivo excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df_plan, df_action, df_speciality, filtered_data = gs1.process_data(filename=filename,valores=valores,regimen=regimen)      \n",
    "######################    \n",
    "\n",
    "#####################\n",
    "#with pd.ExcelWriter(\"output/\"+salida.value) as writer:\n",
    "#   df_action.to_excel(writer, sheet_name='actions')\n",
    "#    df_plan.to_excel(writer, sheet_name='plans')\n",
    "#    df_speciality.to_excel(writer, sheet_name='specialties')\n",
    "#    df_activities.to_excel(writer, sheet_name='activities')\n",
    "\n",
    "# gs1.save_to_excel(output_path=salida.value,valores=valores,regimen=regimen,filename=\"mix_plan.csv\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20List.html\\nimport warnings\\nimport pandas as pd\\nimport ipywidgets as widgets\\nfrom ipywidgets import Button, Layout\\nfrom IPython.display import display\\n\\n#warnings.simplefilter(action=\\'ignore\\', category=FutureWarning)\\n\\nvalores = {\\n    \"D\": 1,\\n    \"S\": 1,\\n    \"2S\": 2,\\n    \"M\": 5,\\n    \"MC\": 1,\\n    \"2M\": 2,\\n    \"T\": 3,\\n    \"SE\": 6,\\n    \"8M\": 8,\\n    \"A\": 1,\\n    \"1.5A\": 18,\\n    \"2A\": 2,\\n    \"3A\": 3,\\n    \"4A\": 4,\\n    \"5A\": 5,\\n    \"6A\": 6,\\n    \"8A\": 8,\\n    \"10A\": 10,\\n    \"1000\": 1000,\\n    \"1300\": 1300,\\n    \"1800\": 1800,\\n    \"6000\": 6000,\\n    \"22500\": 6000,\\n    \"40000\": 40000,\\n    \"55000\": 55000,\\n    \"55000C\": 55000\\n}\\n\\nregimen = {\\n    \"D\": \\'dia\\',\\n    \"S\": \\'semana\\',\\n    \"2S\": \\'semana\\',\\n    \"M\": \\'semana\\',\\n    \"MC\": \\'mes\\',\\n    \"2M\": \\'mes\\',\\n    \"T\": \\'mes\\',\\n    \"SE\": \\'mes\\',\\n    \"8M\": \\'mes\\',\\n    \"A\": \\'Año\\',\\n    \"1.5A\": \\'mes\\',\\n    \"2A\": \\'Año\\',\\n    \"3A\": \\'Año\\',\\n    \"4A\": \\'Año\\',\\n    \"5A\": \\'Año\\',\\n    \"6A\": \\'Año\\',\\n    \"8A\": \\'Año\\',\\n    \"10A\": \\'Año\\',\\n    \"1000\": \\'horas\\',\\n    \"1300\": \\'horas\\',\\n    \"1800\": \\'horas\\',\\n    \"6000\": \\'horas\\',\\n    \"22500\": \\'horas\\',\\n    \"40000\": \\'horas\\',\\n    \"55000\": \\'horas\\',\\n    \"55000C\": \\'ciclos\\'\\n}\\n\\nlad = widgets.Textarea(value=\\'https://docs.google.com/spreadsheets/d/1oUHkuKpHtuhMirNW6SvAQ4A0ns5PZs71iZ_WFXZHNn8/edit?gid=1199302294\\',placeholder=\\'Plan Maestro LAD\\',description=\\'Lineas Alta Demanda:\\',disabled=False,layout=Layout(width=\\'70%\\',height=\"200px\"))\\nlbd = widgets.Textarea(value=\\'https://docs.google.com/spreadsheets/d/1yOaSeqRBr1FW6tvFMi_Y-s4011cKBoyiWU5dTMlujrU/edit?gid=1199302294\\',placeholder=\\'Plan Maestro LBD\\',description=\\'Lineas Baja Demanda:\\',disabled=False,layout=Layout(width=\\'70%\\',height=\"200px\"))\\nhost = widgets.Text(value=\\'192.168.100.50\\',placeholder=\\'Host\\',description=\\'Host:\\',disabled=False)\\nbasedatos = widgets.Text(value=\\'simyo2\\',placeholder=\\'BaseDatos\\',description=\\'BaseDatos\\',disabled=False)\\nusuario = widgets.Text(value=\\'mantto\\',description=\\'Usuario\\')\\npassword = widgets.Password(value=\\'Sistemas0\\',description=\\'Password\\')\\nbutton1 = widgets.Button(description=\"Generar Archivo Excel\",button_style=\\'success\\',layout=Layout(width=\\'20%\\'))\\nbutton2 = widgets.Button(description=\"Cargar en Base de datos\",button_style=\\'danger\\',layout=Layout(width=\\'20%\\'))\\noutput = widgets.Output()\\nsalida = widgets.Text(value=\"Salida.xlsx\",description=\"Nombre:\",disabled=False)\\naccordion = widgets.Accordion(children=[ salida], titles=([\\'Archivo Salida\\']))\\naccordion1 = widgets.Accordion(children=[ usuario,password,host,basedatos], titles=(\\'Usuario\\',\\'Password\\',\\'Host\\',\\'Base de Datos\\'))\\n\\ndisplay(lad,lbd,host,usuario,password,accordion,button1, accordion1,button2,output)\\n\\ndef on_button_clicked(b):    \\n    # Combinacion de dataframes\\n    gs1 = GoogleSheetProcessor(lad.value) #(\"https://docs.google.com/spreadsheets/d/1oUHkuKpHtuhMirNW6SvAQ4A0ns5PZs71iZ_WFXZHNn8/edit?gid=1199302294\")\\n    archivo = \"input/pad.csv\"\\n    #gs.download_csv(archivo)\\n    df1 = gs1.read_csv(archivo)\\n\\n    gs2 = GoogleSheetProcessor (lbd.value) #(\"https://docs.google.com/spreadsheets/d/1yOaSeqRBr1FW6tvFMi_Y-s4011cKBoyiWU5dTMlujrU/edit?gid=1199302294\")\\n    archivo = \"input/pbd.csv\"\\n    #gs.download_csv(archivo)\\n    df2 = gs2.read_csv(archivo)\\n    # Realizar el merge de ambos planes en un solo dataframe\\n    df_merged = pd.concat([df1,df2],ignore_index=True).reset_index(drop=True)\\n\\n    filename = \"input/mix_plan.csv\"\\n    df_merged.to_csv(filename)\\n    \\n    #df_plan, df_action, df_speciality, filtered_data = gs1.process_data(filename=filename,valores=valores,regimen=regimen)      \\n######################    \\n\\n#####################\\n\\n    df_plan, df_action, df_speciality, filtered_data = gs1.process_data(filename=filename,valores=valores,regimen=regimen)  \\n    with pd.ExcelWriter(\"output/\"+salida.value) as writer:\\n        df_action.to_excel(writer, sheet_name=\\'actions\\')\\n        df_plan.to_excel(writer, sheet_name=\\'plans\\')\\n        df_speciality.to_excel(writer, sheet_name=\\'specialties\\')\\n        filtered_data.to_excel(writer, sheet_name=\\'activities\\')\\n    \\n    # gs1.save_to_excel(output_path=salida.value,valores=valores,regimen=regimen,filename=\"mix_plan.csv\")        \\n    with output:\\n        print(\"Se Genera archivo excel Salida.xlsx\")\\n\\nbutton1.on_click(on_button_clicked)\\nbutton2.on_click(lambda _: print(\"Boton 2 accionado\"))\\n#https://ipywidgets.readthedocs.io/en/7.6.3/examples/Widget%20Styling.html '"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' # https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20List.html\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Button, Layout\n",
    "from IPython.display import display\n",
    "\n",
    "#warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "valores = {\n",
    "    \"D\": 1,\n",
    "    \"S\": 1,\n",
    "    \"2S\": 2,\n",
    "    \"M\": 5,\n",
    "    \"MC\": 1,\n",
    "    \"2M\": 2,\n",
    "    \"T\": 3,\n",
    "    \"SE\": 6,\n",
    "    \"8M\": 8,\n",
    "    \"A\": 1,\n",
    "    \"1.5A\": 18,\n",
    "    \"2A\": 2,\n",
    "    \"3A\": 3,\n",
    "    \"4A\": 4,\n",
    "    \"5A\": 5,\n",
    "    \"6A\": 6,\n",
    "    \"8A\": 8,\n",
    "    \"10A\": 10,\n",
    "    \"1000\": 1000,\n",
    "    \"1300\": 1300,\n",
    "    \"1800\": 1800,\n",
    "    \"6000\": 6000,\n",
    "    \"22500\": 6000,\n",
    "    \"40000\": 40000,\n",
    "    \"55000\": 55000,\n",
    "    \"55000C\": 55000\n",
    "}\n",
    "\n",
    "regimen = {\n",
    "    \"D\": 'dia',\n",
    "    \"S\": 'semana',\n",
    "    \"2S\": 'semana',\n",
    "    \"M\": 'semana',\n",
    "    \"MC\": 'mes',\n",
    "    \"2M\": 'mes',\n",
    "    \"T\": 'mes',\n",
    "    \"SE\": 'mes',\n",
    "    \"8M\": 'mes',\n",
    "    \"A\": 'Año',\n",
    "    \"1.5A\": 'mes',\n",
    "    \"2A\": 'Año',\n",
    "    \"3A\": 'Año',\n",
    "    \"4A\": 'Año',\n",
    "    \"5A\": 'Año',\n",
    "    \"6A\": 'Año',\n",
    "    \"8A\": 'Año',\n",
    "    \"10A\": 'Año',\n",
    "    \"1000\": 'horas',\n",
    "    \"1300\": 'horas',\n",
    "    \"1800\": 'horas',\n",
    "    \"6000\": 'horas',\n",
    "    \"22500\": 'horas',\n",
    "    \"40000\": 'horas',\n",
    "    \"55000\": 'horas',\n",
    "    \"55000C\": 'ciclos'\n",
    "}\n",
    "\n",
    "lad = widgets.Textarea(value='https://docs.google.com/spreadsheets/d/1oUHkuKpHtuhMirNW6SvAQ4A0ns5PZs71iZ_WFXZHNn8/edit?gid=1199302294',placeholder='Plan Maestro LAD',description='Lineas Alta Demanda:',disabled=False,layout=Layout(width='70%',height=\"200px\"))\n",
    "lbd = widgets.Textarea(value='https://docs.google.com/spreadsheets/d/1yOaSeqRBr1FW6tvFMi_Y-s4011cKBoyiWU5dTMlujrU/edit?gid=1199302294',placeholder='Plan Maestro LBD',description='Lineas Baja Demanda:',disabled=False,layout=Layout(width='70%',height=\"200px\"))\n",
    "host = widgets.Text(value='192.168.100.50',placeholder='Host',description='Host:',disabled=False)\n",
    "basedatos = widgets.Text(value='simyo2',placeholder='BaseDatos',description='BaseDatos',disabled=False)\n",
    "usuario = widgets.Text(value='mantto',description='Usuario')\n",
    "password = widgets.Password(value='Sistemas0',description='Password')\n",
    "button1 = widgets.Button(description=\"Generar Archivo Excel\",button_style='success',layout=Layout(width='20%'))\n",
    "button2 = widgets.Button(description=\"Cargar en Base de datos\",button_style='danger',layout=Layout(width='20%'))\n",
    "output = widgets.Output()\n",
    "salida = widgets.Text(value=\"Salida.xlsx\",description=\"Nombre:\",disabled=False)\n",
    "accordion = widgets.Accordion(children=[ salida], titles=(['Archivo Salida']))\n",
    "accordion1 = widgets.Accordion(children=[ usuario,password,host,basedatos], titles=('Usuario','Password','Host','Base de Datos'))\n",
    "\n",
    "display(lad,lbd,host,usuario,password,accordion,button1, accordion1,button2,output)\n",
    "\n",
    "def on_button_clicked(b):    \n",
    "    # Combinacion de dataframes\n",
    "    gs1 = GoogleSheetProcessor(lad.value) #(\"https://docs.google.com/spreadsheets/d/1oUHkuKpHtuhMirNW6SvAQ4A0ns5PZs71iZ_WFXZHNn8/edit?gid=1199302294\")\n",
    "    archivo = \"input/pad.csv\"\n",
    "    #gs.download_csv(archivo)\n",
    "    df1 = gs1.read_csv(archivo)\n",
    "\n",
    "    gs2 = GoogleSheetProcessor (lbd.value) #(\"https://docs.google.com/spreadsheets/d/1yOaSeqRBr1FW6tvFMi_Y-s4011cKBoyiWU5dTMlujrU/edit?gid=1199302294\")\n",
    "    archivo = \"input/pbd.csv\"\n",
    "    #gs.download_csv(archivo)\n",
    "    df2 = gs2.read_csv(archivo)\n",
    "    # Realizar el merge de ambos planes en un solo dataframe\n",
    "    df_merged = pd.concat([df1,df2],ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "    filename = \"input/mix_plan.csv\"\n",
    "    df_merged.to_csv(filename)\n",
    "    \n",
    "    #df_plan, df_action, df_speciality, filtered_data = gs1.process_data(filename=filename,valores=valores,regimen=regimen)      \n",
    "######################    \n",
    "\n",
    "#####################\n",
    "\n",
    "    df_plan, df_action, df_speciality, filtered_data = gs1.process_data(filename=filename,valores=valores,regimen=regimen)  \n",
    "    with pd.ExcelWriter(\"output/\"+salida.value) as writer:\n",
    "        df_action.to_excel(writer, sheet_name='actions')\n",
    "        df_plan.to_excel(writer, sheet_name='plans')\n",
    "        df_speciality.to_excel(writer, sheet_name='specialties')\n",
    "        filtered_data.to_excel(writer, sheet_name='activities')\n",
    "    \n",
    "    # gs1.save_to_excel(output_path=salida.value,valores=valores,regimen=regimen,filename=\"mix_plan.csv\")        \n",
    "    with output:\n",
    "        print(\"Se Genera archivo excel Salida.xlsx\")\n",
    "\n",
    "button1.on_click(on_button_clicked)\n",
    "button2.on_click(lambda _: print(\"Boton 2 accionado\"))\n",
    "#https://ipywidgets.readthedocs.io/en/7.6.3/examples/Widget%20Styling.html '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las columnas y sus tipos de datos de la columna activities\n",
    "#query = \"\"\"\n",
    "#SELECT column_name\n",
    "#FROM information_schema.columns\n",
    "#WHERE table_name = 'activities' order by ordinal_position asc;\n",
    "#\"\"\"\n",
    "#df_columnas_activities = ejecutar_query(query)\n",
    "#df_columnas_activities['column_name']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
